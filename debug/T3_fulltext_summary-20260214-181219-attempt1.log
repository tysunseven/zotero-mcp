OpenAI Codex v0.101.0 (research preview)
--------
workdir: C:\Users\28613
model: gpt-5.3-codex
provider: rightcode
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019c5ba3-b152-7f61-9c75-405d9596ae1d
--------
user
Using Zotero MCP tools only: (1) run zotero_search_items query 'deep learning' limit 1 and get item key; (2) run zotero_get_item_fulltext for that key; (3) output a structured summary with headings Problem, Method, Results, Limitations (one short bullet each) and include the item key on first line as 'item_key: ...'.
mcp: zotero starting
mcp: zotero ready
mcp startup: ready: zotero

thinking
**Starting Zotero item retrieval**
codex
I’ll use the Zotero MCP tools in sequence: first find one `deep learning` item and its key, then fetch that item’s full text, then produce the 4-part short summary you requested.
tool zotero.zotero_search_items({"query":"deep learning","limit":"1"})
zotero.zotero_search_items({"query":"deep learning","limit":"1"}) success in 2.30s:
{
  "content": [
    {
      "type": "text",
      "text": "# Search Results for 'deep learning'\n\n\n## 1. Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data\n**Type:** journalArticle\n**Item Key:** 2P33W7SZ\n**Date:** 04/2020\n**Authors:** Sun, Luning; Gao, Han; Pan, Shaowu; Wang, Jian-Xun\n**Abstract:** Numerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Du...\n"
    }
  ],
  "structuredContent": {
    "result": "# Search Results for 'deep learning'\n\n\n## 1. Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data\n**Type:** journalArticle\n**Item Key:** 2P33W7SZ\n**Date:** 04/2020\n**Authors:** Sun, Luning; Gao, Han; Pan, Shaowu; Wang, Jian-Xun\n**Abstract:** Numerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Du...\n"
  },
  "isError": false
}

thinking
**Fetching full text and updating commentary**
codex
I found one matching item and extracted its key; next I’m fetching the full text for that key to summarize it in the required format.
tool zotero.zotero_get_item_fulltext({"item_key":"2P33W7SZ"})
zotero.zotero_get_item_fulltext({"item_key":"2P33W7SZ"}) success in 13.19s:
{
  "content": [
    {
      "type": "text",
      "text": "# Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data\n\n**Type:** journalArticle\n\n**Item Key:** 2P33W7SZ\n\n**Date:** 04/2020\n\n**Authors:** Sun, Luning; Gao, Han; Pan, Shaowu; Wang, Jian-Xun\n\n**Journal:** Computer Methods in Applied Mechanics and Engineering, Volume 361, Pages 112732\n\n**DOI:** 10.1016/j.cma.2019.112732\n\n**URL:** https://linkinghub.elsevier.com/retrieve/pii/S004578251930622X\n\n\n\n## Extra\n\nTLDR: A physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data is provided, and results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\n\n\n\n## Abstract\n\nNumerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics and sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time applications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty quantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL) has shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality. However, the off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and interpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient in most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical simulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural network (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential equations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments are conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties in fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\n\n**Notes/Attachments:** 1\n\n---\n\n## Full Text\n\nAvailable online at www.sciencedirect.com\nScienceDirect\n\nComput. Methods Appl. Mech. Engrg. 361 (2020) 112732\n\nwww.elsevier.com/locate/cma\n\nSurrogate modeling for fluid flows based on physics-constrained deep\nlearning without simulation data\nLuning Suna,b, Han Gaoa,b, Shaowu Panc, Jian-Xun Wanga,b,∗\na Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, United States of America\nb Center for Informatics and Computational Science, University of Notre Dame, Notre Dame, IN, United States of America\nc Department of Aerospace Engineering, University of Michigan, Ann Arbor, MI, United States of America\n\nReceived 18 July 2019; received in revised form 4 November 2019; accepted 4 November 2019\nAvailable online 21 November 2019\n\nAbstract\n\nNumerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the\ngoverning equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics\nand sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time\napplications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty\nquantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL)\nhas shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality.\nthe off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and\nHowever,\ninterpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient\nin most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical\nsimulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL\napproach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural\nnetwork (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential\nequations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments\nare conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties\nin fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and\nforward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\nc⃝ 2019 Elsevier B.V. All rights reserved.\n\nKeywords: Physics-informed machine learning; Label-free; Neural networks; Uncertainty quantification; Cardiovascular flows; Navier-Stokes\n\n1. Introduction\n\nComplex fluids are ubiquitous in natural and industrial processes, and accurately simulating the fluid flows is\nindispensable in many disciplines, e.g., aerospace, civil, and biomedical engineering. A fluid system is typically\ngoverned by the Navier–Stokes equations, which is a highly nonlinear partial differential equation (PDE) system.\n\n∗ Corresponding author at: Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, United\n\nStates of America.\n\nE-mail address:\n\njwang33@nd.edu (J.-X. Wang).\n\nhttps://doi.org/10.1016/j.cma.2019.112732\n0045-7825/ c⃝ 2019 Elsevier B.V. All rights reserved.\n\n\f2\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nNumerical simulation on fluid dynamics problems primarily relies on solving the PDE systems in a discretized\nform using, e.g., finite difference (FD), finite volume (FV), or finite element (FE) methods, which is known\nas the computational fluid dynamics (CFD) approach [1]. However, CFD simulations are often computationally\ncumbersome, especially for the flows with turbulence and complex geometries. Moreover, mesh generation also\nusually incurs a huge burden, in particular when moving boundary or large geometric variation is considered. The\nconsiderable computational expense greatly limits the use of principled CFD model to real-time predictions and\nmany-query analysis, which are highly relevant to many scientific problems and real-life applications, e.g., timely\nclinical diagnosis and surgery planning for cardiovascular diseases, optimization design of aircraft with large\nparameter variations, and forward/inverse uncertainty quantification (UQ) in high-consequence systems. As an\nis a computationally feasible way to tackle the aforementioned\nalternative, a cost-effective surrogate model\nchallenges.\n\nA surrogate model only approximates the input–output relation of a system, which can be evaluated efficiently.\nNamely, given input parameters, e.g., initial/boundary/operational conditions, the quantities of interest (QoIs), such\nas velocity, pressure, shear stress, and their integrals can be obtained rapidly without conducting the principled CFD\nsimulations. The existing surrogate modeling approaches can be roughly categorized into two classes: projection-\nbased reduced order models (ROMs) and data-fit models [2]. In projection-based ROMs, a reduced basis is extracted\nfrom the simulation data using an unsupervised learning technique, e.g., proper orthogonal decomposition (POD,\nalso known as principal component analysis) [3], and the full-order PDE operator is projected onto the subspace\nspanned by the reduced basis. As a result, the degrees of freedom of the system can be significantly reduced,\nand meanwhile, the underlying structure of the full-order model can be retained to a certain extent. Although\nholding some promises, the current projection-based ROM techniques have had limited impact on complex fluid\ndynamic problems mainly because of the stability and robustness issues [4,5]. Moreover, projection-based ROMs\nare highly code-intrusive and their speedup potential is limited when strong nonlinearity exists [6] though several\nremedies such as sparse sampling [7,8] exist. Another way to enable rapid simulations is to build a data-fit model,\nwhere a response surface of the system is learned from the simulation data in a supervised manner. Namely, a\ndeterministic or probabilistic input–output mapping is constructed using, e.g., polynomial basis functions [9], radial\nbasis functions [10], Gaussian process (GP) [11,12], and stochastic polynomial chaos expansion (PCE) [13–15],\namong others. All these models are built upon the CFD solutions of selected collocation points in parameter\nspace without the need to modify the codes of the CFD solver. Because of the non-intrusive feature and ease of\nimplementation, data-fit surrogates have been used for a wide range of forward and inverse uncertainty quantification\n(UQ) problems in fluid dynamics [14,16,17]. However, traditional data-fit models have a hard time handling\nthe problems with strong nonlinearities and high dimensionality. Deep learning (DL), in particular, the deep\nneural network (DNN) has become a popular surrogate modeling approach and has shown great potential to deal\nwith high-dimensional nonlinear UQ problems [18–20]. It has been shown that DNN as a universal function\napproximator [21] can overcome the curse of dimensionality in certain problems [22–24]. In broader scientific\ncomputing and physical modeling communities, machine learning (ML) has been receiving a lot of attentions\n[25–31]. However, the tremendous success of DL in the computer science, witnessed in areas of computer vision\nand image recognition [32], can be mainly attributed to the availability of large-scale labeled data (i.e., “big data”)\nand the interpolatory nature of their problems. Unfortunately, labeled data for surrogate modeling of fluid systems\nare often sparse and could be noisy, since they are obtained from either principled CFD simulations or experimental\nobservations, both of which are expensive to obtain. Therefore, in such “small data” regimes [33], the true power\nof DL cannot be fully exploited by naively using the off-the-shelf DL model in the computer science community\nas an end-to-end fashion [34] for a data-fit of surrogate modeling.\n\nIn conventional ML problems, the mechanism behind the system is usually unknown and thus can only be learned\nfrom the labeled data. In contrast, for modeling a physical system, the governing equations are usually known a\npriori but are difficult to solve efficiently. Instead of learning solely from the labeled data, e.g., solution of the\nstates on certain points in parameter space, the known governing equations can be utilized to constrain (or even\ndrive) the learning to compensate for the insufficiency of the data. Specifically, the training (optimization) of a DNN\ncan be driven by minimizing the residual of the governing equations constructed by the DNN ansatz. This idea of\nphysics-constrained learning is not new and was proposed back in the late ’90s in the context of solving classic\ndifferential equations [35–37]. However, limited by the NN techniques and computational power at that time, this\nseminal work did not have a big impact. Recently, this idea has been revived because of the recent advances in\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n3\n\ndeep learning [32] combined with ever-increasing computational resources. Notably, the physics-informed neural\nnetwork (PINN) proposed by Raissi et al. was used to solve a number of deterministic one-dimensional (1D) PDEs\nsuch as viscous Burger’s equation, and two/three-dimensional (2D/3D) PDE-constrained inverse problems with a\nmoderate amount of labeled data [33,38,39], e.g., measurements of the velocity field. A similar approach is also\napplied to learn the constitutive relationship in a Darcy flow [40]. The PINN approach has been recently extended\nto assimilate multi-fidelity training data [41], and its UQ analyses have been explored based on arbitrary polynomial\nchaos [42] and adversarial inference [43]. Similar ideas of using physical constraints to regularize the DNN training\nhave also been investigated in [44–47]. In the aforementioned works, a moderate amount of labeled data either\nfrom simulations or experimental measurements are still needed for obtaining an approximation to the solution of\nthe PDEs. In fact, if the initial and boundary conditions are well imposed thus the corresponding PDE problem\nis well-defined, in principle, the unique solution should be captured by the DNN via PDE-constrained learning\nwithout any labeled data. This is one of the main motivations of current work. Note that recently there have been\nseveral works on the concept of data-free DNNs, e.g., for solving a handful of computer vision problems [48],\ndeterministic PDEs [33,49,50], high-dimensional stochastic partial differential equations (SPDE), and backward\nstochastic differential equations (BSDE) [51–55].\n\nIn the context of surrogate modeling, Nabian and Meidani [56] and Karumuri et al. [57] applied the PDE-\nconstrained fully-connected neural network (FC-NN) for uncertainty propagation in steady heat equations. Zhu\net al. [58] proposed a PDE-constrained, label-free DNN surrogate model for UQ in an elliptic PDE using both the\nFC-NN and convolutional neural networks (CNN). Moreover, both the deterministic and probabilistic formulations\nof physics-constrained learning were studied. Their results have shown a significant potential of using the physics-\nconstrained DNN for surrogate modeling, where no labeled data are required during the training. Nonetheless, the\nsuccess has only been demonstrated in a number of canonical problems with regular (rectangular) geometries, and it\nremains unclear if the physics-constrained learning can handle realistic fluid systems governed by the Navier–Stokes\nequations in a parametric setting. Significant work is still needed to further explore the real-world problems for broad\nimpacts.\n\nThe objective of this paper is to develop a physics-constrained, data-free DNN for surrogate modeling of\nincompressible flows. A structured FC-NN is devised to approximate the solutions of parametric Navier–Stokes\nequations, where the initial/boundary conditions are enforced instead of being penalized together during training\nin previous works [33]. In addition, contrary to the existing data-driven surrogates, the training of our DNN is\nsolely driven by minimizing the residuals of the governing PDEs (i.e., conservation laws), where no expensive CFD\nsimulation data is used. The effectiveness and merits of the proposed method are demonstrated by investigating\na number of internal flows relevant to cardiovascular applications. The contributions of the current paper are\nsummarized as follows. First, this work explored the performance of label-free deep learning for parametric surrogate\nconstruction, while most existing PINN works focus on solving deterministic PDEs in a non-parametric setting [33].\nSecond, in this study a more challenging problem is tackled, where the system is governed by the full Navier–Stokes\nequations and with irregular geometries. Particularly, we demonstrated that the flow solutions to geometric variations\ncan be accurately captured by the proposed deep learning approach, showing good promise for rapid geometric\noptimization and uncertainty quantification. To the best of authors’ knowledge, this is the first attempt of using a\nsingle DNN structure to learn the solutions of Navier–Stokes equations in a parametric setting without relying on\nany labeled training data. Lastly, the boundary conditions are encoded into the DNN architecture in a hard manner,\nand the advantages of using “hard” boundary enforcement compared to the “soft” ones are demonstrated in data-\nfree scenarios. The current work aims to push forward the PDE-constrained deep learning framework towards more\nrealistic applications. The rest of the paper is organized as follows. The framework of structured FC-NN surrogate\nbased on the physics-constrained label-free training is introduced in Section 2. Numerical results of surrogate\nmodeling and uncertainty propagation on several vascular flows are presented in Section 3. The performance of\nsoft and hard boundary enforcement approaches, different adaptive activation functions, and data-free/data-driven\nlearning strategies are discussed in Section 4. Finally, conclusion is drawn in Section 5.\n\n\f4\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 1. A schematic diagram of the physics-constrained, data-free DL framework for surrogate modeling of fluid flows. A structured fully-\nconnected neural network (FC-NN) is developed with the boundary conditions encoded by construction. The network is trained by minimizing\nthe equation-based loss function and no CFD simulation data are needed.\n\n2. Methodology\n\n2.1. Overview\n\nMost low-speed flows, e.g., blood flows in large or medium sized vessels, can be described by the incompressible\n\nNavier–Stokes equations given as:\n∇ · u = 0,\n∂u\n∂t\n\nF (u, p) = 0 :=\n\n⎧\n⎨\n\n⎩\n\n+ (u · ∇)u +\n\nx, t ∈ Ω f,t , θ ∈ Rd ,\n\n1\nρ\n\n∇ p − ν∇2u + b f = 0,\n\nx, t ∈ Ω f,t , θ ∈ Rd\n\n(1)\n\nwhere t and x are time and space coordinates, respectively; Ω f,t ≜ Ω f × [0, T ]; θ is a d-dimensional parameter\nvector, including input and/or operational parameters such as fluid properties, inlets/outlets, and geometry of the\ndomain; both velocity u and pressure p are functions of time t, space x, variable parameters θ ; ρ and ν represent\nis the body force; Ω f ⊂ R3 denotes the fluid domain. The\ndensity and viscosity of the fluid, respectively; b f\nsolutions of velocity u and pressure p can be uniquely determined when suitable initial and boundary conditions\nare prescribed,\n\nI(x, p, u, θ ) = 0,\nB(t, x, p, u, θ ) = 0,\n\nx ∈ Ω f , t = 0, θ ∈ Rd ,\nx, t ∈ ∂Ω f × [0, T ], θ ∈ Rd ,\n\n(2a)\n\n(2b)\n\nwhere both I and B are general differential operators that define the initial and boundary conditions, respectively;\n∂Ω f denotes the boundary region. When a set of parameters θ is given, the flow fields, i.e., u(t, x) and p(t, x), can\nbe solved numerically by discretizing the Eqs. (1) and (2) using FD/FV/FE methods. However, this process involves\nmesh generation and iteratively solving large linear/nonlinear systems, which is usually time-consuming. Therefore,\npropagating the parameter uncertainty or inferring the unknown parameters through the FD/FV/FE solver becomes\nintractable when it comes to parametric problems, e.g., some parameters of θ are uncertain or unknown. Solving\nvarying-geometry problems is especially challenging since any change of the geometry requires regeneration of the\ncomputational meshes.\n\nTo enable fast predictions in terms of UQ and optimization applications, a deep neural network (DNN)\narchitecture is built to approximate the solutions of the Navier–Stokes equations in a parametric setting. The DNN-\nbased surrogate is expected to provide a rapid online prediction of the flow field with any given set of parameters θ\nafter the offline training. A schematic diagram of the proposed framework is shown in Fig. 1. A FC-NN is devised\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n5\n\nwith the input layer composed of time t, spatial coordinates x, and variable parameters θ . The raw outputs of the\nFC-NN (i.e., u and p) are used to construct the state variables (i.e., velocity ˆu and pressure ˆp) together with\ncontribution from the particular solution that encodes initial/boundary conditions. The FC-NN is trained by mini-\nmizing residuals of the Navier–Stokes equations and no data from CFD simulations are needed. Therefore, the DNN\npredictions are expected to conform to the conservation laws of fluid flows and satisfy the specified initial/boundary\nconditions. Note that the Navier–Stokes equations will not be solved with any numerical discretization. The details of\nthe physics-constrained training and boundary condition enforcement will be presented in the following subsections.\n\n2.2. Deep neural network and physics-constrained training\n\nNeural networks (NN) are a set of algorithms, inspired by the biological neural networks in brains, for\nclassification and regression tasks. There are various types of NNs with different neuron connection forms and\narchitectures, e.g., fully-connected neural networks (FC-NN), convolutional neural networks (CNN), and recurrent\nneural networks (RNN). In this work, the feedforward FC-NN is considered, where the neurons of adjacent layers\nare fully connected and outputs of each layer are fed forward as the inputs to the next layer. A FC-NN defines a\nmapping from the input layer z0 ∈ Rn0 to the output zL ∈ Rn L . The layers between the input and output layers are\ncalled hidden layers zl, where l = 1, . . . , L − 1. By convention, a neural network with more than one hidden layer\nis called a “deep” NN. Mathematically, two adjacent layers are connected as,\n\nzl = σl(WT\n\nl zl−1 + bl),\n\n(3)\nwhere Wl ∈ Rnl−1×nl and bl ∈ Rnl are the weight matrix and bias vector; the subscript l denotes the index\nof the layer; σl(·) is an activation function acting element-wise, for which a number of options can be chosen,\ne.g., sigmoids, rectified linear units (ReLU), and tanh functions. After training, the weights, bias, and activation\nfunction at each layer are determined, and the output prediction zL (i.e., velocity and pressure) can be rapidly\ncomputed from any given input vector z0 (i.e., coordinates and parameters) based on the Eq. (3). Since this\nfeedforward algorithm (Eq. (3)) only involves a few matrix multiplications, the computational cost for evaluating\nthe trained FC-NN can be neglected compared to that of a CFD simulation.\n\nTraditionally, to build a surrogate model for the CFD simulation of the solution f(t, x, θ ), one can simply consider\n\na black-box surrogate, e.g., FC-NN, or CNN [18], as zL (t, x, θ ; W, b), i.e.,\n\n(4)\n\nf(t, x, θ ) ≈ ˜f(t, x, θ ) ≜ zL (t, x, θ ; W, b),\np]⊺\n\nwhere f is the solution vector as f = [u\n, including velocity u and pressure p; W and b denote the weights and\nbiases of the entire network. Generally, training of a DNN is purely data-driven, and it consists of finding a set of\n(sub)optimal DNN parameters (W, b) such that the mismatch between the training data fd and the DNN predictions\n˜f is locally minimized. That is, one can formulate an optimization problem as,\n\nLdata(W, b) = \n\nfd (t, x, θ ) − zL (t, x, θ ; W, b)\nLdata(W, b),\n\nΩ f,t\n\n,\n\n(5a)\n\n(5b)\n\nW∗, b∗ = arg min\n\nW,b\n\nwhere the loss function Ldata(W, b) is named as “data-based loss”, and ∥·∥Ω f,t is L 2 norm over Ω f,t ; W∗, b∗ denote\na set of (sub)optimal NN weights and biases obtained from the optimization.\n\nHowever, as discussed above, this black-box surrogate modeling requires enormous training data fd , which is too\nexpensive to obtain from a large number of CFD simulations. Instead, following previous PINN framework [33], we\nconsider leveraging the governing PDEs in the loss function by minimizing the violation of the solution zL in terms\nof the known governing PDEs for fluid dynamics over a domain of interests without the needs of solving these\nequations for each parameter with traditional numerical methods. Specifically, only the residuals of the Navier–\nStokes equations are computed based on the FC-NN predictions and it corresponds to a constrained optimization\nas follows,\n\n∇ · ˜u\nL phy(W, b) = \nΩ f,t\n\n\n\nMass conservation\n\n+ ∥\n\n\n\n∂ ˜u\n∂t\n\n1\nρ\n\n+ ( ˜u · ∇) ˜u +\n\n∇ ˜p − ν∇2 ˜u + b f ∥Ω f,t\n\n\nMomentum conservation\n\n,\n\n(6)\n\n\f6\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nW∗ = arg min\n\nL phy(W),\n\n{\n\ns.t.\n\nW\n\nI(x, ˜p, ˜u, θ ) = 0,\nB(t, x, ˜p, ˜u, θ ) = 0,\n\nt = 0, in Ω f ,\non ∂Ω f,t ,\n\n(7)\n\nwhere the loss function L phy(W, b) here is named as “physics-based loss”; To construct the PDE residuals in the\nloss function, several first and/or second derivative terms of ˜u and ˜p with respect to time t and space coordinates x\nare required, which can be computed based on automatic differentiation (AD) [59]. AD is an accurate and efficient\nway to calculate derivatives in a computational graph, which has started to gain increasing attention in the machine\nlearning community. The general idea of AD is to use the chain rule to back-propagate derivatives from the output\nlayer to the inputs as the connection between each layer of a NN is analytically defined. Compared to numerical\ndifferentiation techniques, derivatives calculated from AD are much more accurate since they do not suffer from\ntruncation or round-off errors. Most modern deep learning frameworks such as PyTorch [60], TensorFlow [61], and\nTheano [62] have the AD implemented. To solve the optimization problem defined in Eq. (6), stochastic gradient\ndescent (SGD) algorithms are used, which are known to be a stochastic approximation of the gradient descent (GD)\noptimization. In SGD, only a subset of points are randomly sampled from the input space to calculate the direction\nof the gradient at each iteration. The SGD algorithms are known to work very well to escape bad local minima\nin the neural network training [63] under one point convexity property. Although the global minimum cannot be\nguaranteed for a non-convex optimization problem as defined in Eq. (6), an empirically good local minimum is\nusually found based on the SGD algorithms.\n\n2.3. Boundary condition enforcement\n\nIf the physics-based loss L phy becomes identically zero, the DNN predictions of velocity ˜u and pressure ˜p will\nexactly satisfy the Navier–Stokes equations (Eq. (1)). Therefore, penalizing the PDE residuals can regularize the\ndata-driven DNN solutions to be more physical. This idea is known as the physics-informed, weakly-supervised\ndeep learning [33,38,45]. To make the problem well-posed, proper initial and boundary conditions (IC/BC) are\nneeded and imposed as constraints (Eq. (7)) which are often treated in a “soft” manner by modifying the original\nloss function with penalty terms [33,64]. For example, the IC/BC can be imposed in a “soft” way by modifying\nEq. (6) as,\n\nLc\n\nphy(W, b, λi , λb) = L phy(W, b)\n\n\n\nEquation loss\n\n+ λi\n\n\nI(x, ˜p, ˜u, θ )\n\n\nInitial loss\n\nΩ f,t\n\n\n+ λb\n\n\nB(t, x, ˜p, ˜u, θ )\n\n\nBoundary loss\n\n∂Ω f,t\n\n\n,\n\n(8)\n\nwhere λi and λb are penalty coefficients. However, the soft IC/BC enforcement methods have several major\ndrawbacks: (1) there is no quantitative guarantee on how accurate the IC/BC being imposed and thus the solution\ncould be unsatisfactory; (2) the optimization performance can depend on the relative importance of each term, but\nhow to assign weight for each term can be difficult. Alternatively, we can impose the IC/BC in a “hard” manner,\nwhere a particular solution that solely satisfies the initial/boundary condition is added. Hence, the constraints on\nIC/BC are automatically fulfilled. A mixed enforcement on IC/BC is proposed in this work, where the Neumann\nand Dirichlet boundary conditions (BC) are treated separately: the Neumann BC are formulated into the equation\nloss, i.e., in a soft manner, while the IC and Dirichlet BC are encoded in a hard manner by constructing the DNN\nansatz ˆu and ˆp with a particular solution as follows,\n\nˆu(t, x, θ ; W, b) = u par (t, x, θ ) + D(t, x, θ ) ˜u(t, x, θ ; W, b),\nˆp(t, x, θ ; W, b) = p par (t, x, θ ) + D(t, x, θ ) ˜p(t, x, θ ; W, b),\n\n(9)\n\n= ub(x), p par (x, t)|x∈∂Ω f\n\nis a particular solution that just satisfies IC and BC: u par (x, 0) = u0(x), p par (x, 0) = p0(x) and\nwhere u par\nu par (x, t)|x∈∂Ω f\n= pb(x); D(t, x, θ ) is a globally defined smooth function from internal\npoints to the “boundary” in Ω f,t , i.e., a space–time sense. That is, D is zero on the boundary ∂Ω f × [0, T ] and\nΩ f × {0} while increases away from the boundary. For those problems where the IC/BC and the geometry of the\ndomain in Ω f,t is simple, the function D and particular solution can be written analytically. However, if the geometry\nis too complex to have an analytic form, e.g., a patient-specific artery, both the particular solution (u par , p par )\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n7\n\nand smooth distance function (D) can be parameterized by several pre-trained NNs similarly as [50]. These pre-\ntrained NNs can be designed to over-fit the boundary values, since they are only used to represent the particular\nsolutions or distance function as differentiable forms. Finally, the constrained optimization problem in Eq. (6) can\nbe reformulated as an unconstrained one, as shown in Eq. (10).\n\n∇ · ˆu\nphy(W, b) = \nLc\n\n\nMass conservation\nLc\n\nW∗, b∗ = arg min\n\nΩ f\n\n\nW,b\n\n∂ ˆu\n∂t\n\n\nphy(W, b),\n\n+ ∥\n\n+ ( ˆu · ∇) ˆu +\n\n1\n∇ ˆp − ν∇ 2 ˆu + b f ∥Ω f\nρ\n\n\nMomentum conservation\n\n(10a)\n\n(10b)\n\nNote that since the Neumann BCs were not studied numerically in this work, the loss term associated with Neumann\nBC is omitted in Eq. (10) and thus the total loss only contains the L 2 residual of governing equations.\n\n3. Numerical results\n\nA number of 2D vascular flows with idealized geometries, including circular pipe flows, stenotic flows, and\naneurysmal flows, are studied to evaluate the performance of the proposed data-free DL surrogate model in Eq. (10).\nForward propagation of the uncertainties in the fluid properties and domain geometry to the flow QoIs (e.g., velocity,\npressure, and wall shear stress) are investigated through the DNN surrogate model. In this study, only steady-state\nsolutions are considered for proof-of-concept, thus the constraint of initial condition can be neglected in Eq. (9).\n\nA composite FC-NN architecture is devised for the surrogate, which is composed of three sub-DNNs with an\nidentical structure of 3 hidden layers with 20 neurons per layer. The Swish activation function [65], defined as\nx · Sigmoid(βx) with a fixed hyperparameter β = 1, is employed in each layer except the last one, where a linear\nactivation function is used. The three sub-DNNs share the same input layer and separately predict three scalar\nstate variables, i.e., velocity u, v, and pressure p. All three sub-DNNs are trained simultaneously with a unified\nphysics-based loss function. To solve the unconstrained optimization problem defined in Eq. (10), we used the\nAdam optimizer [66], a robust variant of the SGD method, where the learning rate is adaptively changed based on\nthe estimates of the moments. The initial learning rate and mini-batch size are set as 1 × 10−3 and 50, respectively.\nBecause of the adaptivity feature of the Adam optimizer, the hyperparameters in the training are robust to some\nextent and require little tuning. Properly initializing the DNN parameters is also important. After comparing several\nwidely-used initialization schemes [67,68], we chose the He’s normal initializer [68], where initial weights are drawn\nfrom a truncated normal distribution. In general, a good choice of the DNN architecture, including the number of\nlayers, number of neurons per layer, activation function, and initialization schemes, is important to the learning\nperformance but is still determined by trial and error. A rule of thumb is to achieve the desired learning performance\nusing the “simplest” network structure, which is known as “Occam’s razor” [69]. Namely, a DNN structure with\nthe minimum number of layers and neurons still having the desired performance is preferable, because it usually\nenables efficient training and better generalizability. To search for such a structure, a series of tests were conducted.\nSpecifically, we studied the ν-varying stenosis case using a group of DNN structures with different number of hidden\nlayers and neurons, from the shallowest network with only 1 hidden layer of 10 neurons per layer to the largest one\nwith 5 hidden layers of 40 neurons per layer. By comparing the test errors of all cases (Table 5), we found the test\nerrors do not notably decrease with the DNN structures of more than 3 hidden layers and 20 neurons in each layer,\nwhich justified the current choice of the DNN structure. More details can be found in Appendix A. To demonstrate\nthe robustness of the physics-constrained learning, the architecture and hyperparameters remain the same for all the\ncases throughout the paper. Note that a comprehensive parameter study and architecture optimization of the DNN\nare out of the scope of the current work.\n\nThe composite FC-NN is implemented in the PyTorch platform [60]. As discussed in Section 2, only the\ncollocation points are required and they are uniformly sampled in the spatial x and parameter θ spaces. Alternatively,\none can choose a space-filling Latin hypercube sampling in the Ω f [33]. In this work, the PDE residuals are\nextensively evaluated on a large number of collocation points to ensure learning quality. For all test cases, the\ntraining of about 106 SGD iterations are performed on an NVIDIA GeForce GTX 1080 Ti Graphics Processing\nUnit (GPU) card, and the cost is approximately 3.5 h. Note that the offline training cost can be potentially reduced\nby optimizing the DNN hyperparameters. To validate the prediction performance of the trained DNN surrogates,\ncorresponding CFD simulations are also conducted using an open-source FV-based CFD solver, OpenFOAM [70].\nMesh convergence study is performed to ensure the solution accuracy. The code and datasets for this work will\nbecome available at https://github.com/Jianxun-Wang/LabelFree-DNN-Surrogate upon publication.\n\n\f8\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 2. (a) DNN predicted cross-section pipe flow velocity profiles u(y) of four different viscosity (ν) samples compared with the analytical\nsolution; (b) probability density function of the center velocity uc = u(y = 0) propagated from a truncated Gaussian distributed ν with mean\nν = 10−3 and variance of σν = 2.67 × 10−4, using the trained DNN surrogate compared with the analytical solution.\n\n3.1. Circular pipe flow\n\nFlow in a pipe/tube is very common in physiological systems, e.g., blood in arteries or air flow in trachea.\nThe pipe flow is often driven by the pressure difference between the two ends of a tube, or by the body force\nof gravity. In a cardiovascular system, the former one is more dominant since the blood flow is mainly governed\nby pressure drop due to the heart pumping. In general, simulating the fluid dynamics in a tube requires solving\nthe full Navier–Stokes equations numerically, but if the tube is straight and has a constant circular cross section,\nanalytical solution of the fully-developed steady-state flow is available, which is an ideal benchmark to validate the\nperformance of the proposed method. As a result, we first study the flow in a 2D circular pipe (also known as the\nPoiseuille flow).\n\nIn this case, the pressure inlet and outlet are used to drive the flow since we only focus on the fully-developed\nregime, and no-slip wall boundary is prescribed on the tube walls. The boundary conditions are encoded into the\nsurrogate model by constructing the DNN ansatz ˆu, ˆv, and ˆp based on Eq. (9). The no-slip condition of velocity\non the wall can be imposed by designing the ˆu, ˆv as,\n\nˆu =\n\n( d 2\n4\n\n)\n\n˜u,\n\n− y2\n\nˆv =\n\n( d 2\n4\n\n)\n\n˜v,\n\n− y2\n\nwhere y is the radial distance, d = 0.1 is the diameter of the tube, the raw DNN output is denoted by ˜u.\n\nThe pressure inlet pin = 0.1 and outlet pout = 0 are imposed by designing the ˆp as,\n\nˆp =\n\nx − xin\nxout − xin\n\npout +\n\nxout − x\nxout − xin\n\npin + (x − xin)(xout − x) ˜p,\n\n(11)\n\n(12)\n\nwhere xin and xout are coordinates of the two ends of the tube, and the raw DNN output is denoted by ˜p. All\nthree sub-DNNs are trained to capture the spatial flow fields with parameter variation in the fluid viscosity ν. Input\n(collocation) points in the parameter space of ν are uniformly sampled in the range 2 × 10−4 ≤ ν ≤ 1.9 × 10−3,\nwhere the corresponding Reynolds number (Re) are moderate (Re < 300). After training, both velocity and pressure\nfields can be obtained immediately by evaluating the trained DNN with any given input ν and a spatial grid on x.\nHence, the DNN surrogate can be utilized to propagate the uncertainty in viscosity ν based on the Monte Carlo\n(MC) simulation, where a large number of samples are drawn from the ν distribution and propagated to the QoIs\nvia the DNN surrogate. In the following test cases, 500 MC samples are used to compute desired statistics and\ndistributions.\n\nThe DNN surrogate results (shown in Fig. 2) are compared against the analytical solution, which is given by\n\nua =\n\n∆ p\n2νρ L\n\n( d 2\n4\n\n)\n\n,\n\n− y2\n\n(13)\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n9\n\nFig. 3. Learning performance in different ranges of Reynolds number Re. Solid lines indicate interpolation (testing within the training range)\nand dashed lines indicate extrapolation; ν20 and ν50 indicate that the number of collocation points in ν is 20 and 50, respectively. (For\ninterpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\n\nwhere y denotes the spanwise coordinate and ∆ p is the pressure difference. It can be observed from Fig. 2a that\nthe DNN-predicted velocity profiles (red dashed lines) of four different ν samples almost exactly agree with the\nanalytical solutions (blue solid lines), where the Reynolds numbers (Re) of the four cases are 283, 121, 33, and 3,\nrespectively. Actually, the trained DNN is able to accurately predict the pipe flow field with any given viscosity,\nwhere the Reynolds number is moderate. Fig. 2b shows the uncertainty of the center velocity uc propagated from\na truncated Gaussian distributed ν with mean of ν = 1 × 10−3 and variance of σν = 2.67 × 10−4 to guarantee the\nviscosity ν is always positive random variable. Namely, the truncated Gaussian distribution is defined with in the\nrange of (0, +∞), and the probability density function f is shown as,\n\nf (ν; ν, σν) =\n\n1\n\nσν N ( ν−ν\nσν )\n1 − φ(− ν\nσν )\n\n, ν > 0\n\n(14)\n\n∫ π\n0 e−t 2dt. It can be seen\nwhere N is the standard Gaussian density, and φ(x) = 1\n2\nthat the DNN-predicted probability density function (PDF) is almost identical to that of the analytical solutions,\ndemonstrating the excellent performance of the physics-constrained learning for uncertainty propagation.\n\n, with erf(z) = 2√\nπ\n\n1 + erf( x√\n2\n\n(\n\n)\n)\n\nTo comprehensively evaluate the learning performance, mean square errors of the DNN predictions are calculated\nand normalized by ∆ p. It is interesting to examine how well the predictions are across different Reynolds numbers\nRe in both the training range and the range of extrapolation. In current training setting, collocation points of ν are\nsampled from [2 × 10−4, 1.9 × 10−3], corresponding to Re ∈ [10, 300] as the training range. The trained DNN is\nalso tested on Re ∈ (300, 500], which is the range of extrapolation. Moreover, we also investigated the impact of\nRe on the learning performance by using a wider Re range for training. The mean square test error eu for each\ncase are plotted against the Reynolds number Re in Fig. 3, where solid lines indicate interpolation (testing within\nthe training range) and dashed line indicates extrapolation.\n\nIt is clear that the test error in the interpolation range (Re ∈ [10, 300], red solid) is relatively lower than that in\nthe extrapolation range (Re ∈ (300, 500], red dashed). If the training set includes Re ∈ (300, 500], the test error\ncan be largely reduced by nearly an order of magnitude (see the comparison of black solid line and red dashed\nline). This is expected since the success of deep learning largely relies on the interpolatory nature of the problem,\nand the extrapolation is always much more challenging. However, the generalization ability is not critical in the\nlabel-free learning framework since any collocation points can be freely drawn in the range of interest for training.\nMoreover, the effect of the total number of collocation points was studied and we found that the test error can\nbe further reduced if more collocation points are sampled during the training. This can be observed by comparing\nthe blue and black lines, which correspond to the cases trained with 50 and 20 collocation points of ν (or Re),\nrespectively. Lastly, it can be seen that the test error tends to grow as the Re increases, regardless of interpolation or\n\n\f10\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nextrapolation, which indicates that the flow solutions with higher Reynolds are more difficult to be captured by the\nDNN. It might be because steep gradients of flows with higher Re pose more challenges on learning. We refined\nourselves to the laminar flow regime, and higher-Re turbulent flows is out of the scope of this work.\n\n3.2. Blood flow with standardized geometry\n\nTwo types of canonical vascular flows, stenotic flow and aneurysmal flow, with standardized vessel geometries\nare studied. A stenotic flow refers to the flow through a vessel where there is narrowing and re-expansion of the\nvessel wall. This local restriction of the vessel is related to many cardiovascular diseases, e.g., arteriosclerosis,\nstroke, and heart attack [71]. The vascular flow within an aneurysm, which is the expansion of an artery due to\nthe weakness of vessel walls, is called aneurysmal flow. The rupture of an aneurysm may cause life-threatening\nconditions, e.g., subarachnoid hemorrhage (SAH) due to cerebral aneurysm rupture [72], and investigation of the\nhemodynamics can improve the diagnosis and fundamental understanding of aneurysm progression and rupture [73].\nWhereas realistic vascular geometries are usually irregular and complex, including sites of curvature, bifurcation\nand junctions, idealized stenosis and aneurysm models are studied here for proof-of-concept. Namely, both the\nstenotic and aneurysmal vessels are idealized as an axisymmetric tube with a varying cross-section radius, which\nis parameterized by the following function,\n\nR(x) = R0 − A\n\n1\n(2πσ 2)\n\n√\n\nexp(−\n\n(x − µ)2\n2σ 2\n\n)\n\n(15)\n\nwhere R0 is the radius at the inlet, which is set as 0.05, and the sign of A determines if the vessel is stenotic\nor aneurysmal. Namely, a positive and negative sign correspond to the stenosis and aneurysm, respectively. Three\ncontrol parameters A, µ, and σ define the shape of the stenotic (aneurysmal) vessel. The scale parameter A controls\nthe curvature along the tube, and a larger |A| leads to narrower stenosis (broader aneurysm). Parameter µ defines\nthe streamwise location of the minimum (maximum) radius of the stenosis (aneurysm), and σ affects the steepness\nof the geometric variation. In this study, the latter two parameters µ and σ are fixed as 0.5 and 0.1, respectively.\nOnly A is considered as a variable parameter to control the degree of the stenosis (aneurysm).\n\nSimilar to the pipe flow, the pressure inlet/outlet and no-slip wall boundary conditions are prescribed for\nboth stenosis and aneurysm cases. The boundary-encoded sub-DNNs are constructed to learn the parametric flow\nsolutions, where the wall BC is imposed using the geometric function D(x, y) = R(x)2 − y2. Contrary to the case\nstudied above, where only the viscosity variation is considered, the DNN surrogate is also trained to capture the\nvarying stenosis (aneurysm) geometry, which is known challenging for mesh-based CFD simulations. Specifically,\nthe solutions of varying viscosity are learned for fixed vascular geometries (A = 5 × 10−3 for stenosis and\nA = −5 × 10−3 for aneurysm) in the first place, and then the performance for capturing geometry variations\nis examined at a fixed viscosity (ν = 1 × 10−3). Moreover, the uncertainties from the flow viscosity and vessel\ngeometry are propagated to the QoIs through the trained DNN surrogate using MC sampling, and the results are\nvalidated by CFD-based MC simulations.\n\n3.2.1. Flow in idealized stenosis\n\nThe DNN is trained to parameterize the solutions of stenotic flows with varying viscosity, where collocation\n\npoints are sampled in ν space within the range of [5 × 10−4, 1 × 10−2] for physics-based training.\n\nFig. 4 shows the DNN-predicted flow fields of three different viscosity samples, i.e., ν = 6.4×10−4, 1.85×10−3,\nand 2.14×10−3, at moderate Reynolds numbers. The corresponding CFD simulations are performed for comparison.\nIt can be seen that the flow patterns of different ν are similar, where the fluid is accelerated streamwisely through the\nconverging region and slows down passing the diverging part of the tube. However, the velocity magnitude reduces\nas the viscosity increases (left to right columns). As shown in Fig. 4a and b, the DNN-predicted velocity contours\nof streamwise and spanwise components (u and v) agree with the CFD solutions very well, though the magnitude\nin the case with the smallest viscosity (ν = 6.4 × 10−4) is slightly underestimated. Moreover, the nonlinear pressure\ndrops can be accurately captured by the DNN surrogate as the profiles of centerline pressure from the DNN and\nCFD are almost identical (Fig. 4c).\n\nTo learn the flow solutions with varying geometry, the DNN is trained on uniformly sampled points within the\n\nrange of 0 ≤ A ≤ 1 × 10−2.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n11\n\nFig. 4. Comparison between physics-constrained DNN predictions and CFD solutions of idealized stenotic flows at three different viscosity\nsamples: (left) ν = 6.4 × 10−4, (middle) ν = 1.85 × 10−3, (right) ν = 2.14 × 10−3. (a) Streamwise velocity component u (b) spanwise\nvelocity component v (c) centerline pressure profile Pc.\n\nWe compare the flow fields predicted by the trained DNN against the CFD benchmarks, and the results of three\ndifferent samples of stenosis geometries, A = 2 × 10−3, 4 × 10−3, and 7 × 10−3, are shown in Fig. 5. From left to\nright, the degree of stenosis increases and thus the total flow rate is reduced due to the increased resistance (Fig. 5a\nand b). The pressure drop becomes more nonlinear as the stenotic vessel turns to be narrower (Fig. 5c). We can see\nthat the DNN predictions can capture these flow features and agree with the CFD benchmarks well. Admittedly,\nthe streamwise velocity of the flow with the narrowest stenotic vessel is slightly underestimated, and the centerline\npressure profiles predicted by the DNN and CFD has a small discrepancy. This might be because the increased\nnonlinearity due to the steep geometric variation poses a challenge on the learning.\n\nAfter the training, the DNN surrogates are used to rapidly propagate uncertainties in viscosity and vessel\ngeometry, and the effects on QoIs are investigated. Specifically, 500 MC samples are drawn from a truncated\nGaussian distribution of viscosity ν and a Gaussian distribution of geometric parameter A, which are propagated to\nthe center velocities uc (at x = 0.5, y = 0.0) through the DNN surrogates. The probability distributions of uc due\nto uncertain viscosity and vessel geometry are shown in Fig. 6a and b, respectively, where the propagated results\nthrough the CFD solver are also plotted for comparison.\n\nIt shows that the propagated distributions of uc are non-Gaussian in both cases, which is due to the strong\nnonlinearity of the Navier–Stokes operator. As expected,\nthe DNN-propagated uncertainties present a good\nagreement with the CFD-based benchmarks, especially in the case of viscosity uncertainty propagation (Fig. 6a),\nwhere the two PDF curves are almost overlapped with each other. As for the geometry uncertainty propagation,\nalthough the overall feature of the PDF is captured, the peak density is slightly underpredicted by the DNN. The\nreason behind this could be that the DNN surrogate tends to underestimate the velocity magnitude in particular for\na steep geometry variation.\n\n\f12\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 5. Comparison between physics-constrained DNN predictions and CFD solutions of idealized stenotic flows with three different stenosis\ngeometries: (left) A = 2 × 10−3, (middle) A = 4 × 10−3, (right) A = 7 × 10−3. (a) Streamwise velocity component u (b) spanwise velocity\ncomponent v (c) centerline pressure profile Pc.\n\nFig. 6. Probability density of the center velocity uc propagated from (a) a truncated Gaussian distributed viscosity ν and (b) a normally\ndistributed geometric parameter A based on the trained DNN surrogate, compared against CFD-based MC solutions.\n\n3.2.2. Flow in idealized aneurysm\n\nWe first learn the aneurysmal flows with varying viscosity, where the geometry of the aneurysm is fixed\n\n(A = −5 × 10−3).\n\nTraining is conducted by sampling the viscosity points ranging from 5 × 10−4 to 1 × 10−2. Fig. 7 shows the\nDNN-predicted flow fields of three viscosity samples, i.e, ν = 6.4 × 10−4, 1.85 × 10−3, 2.14 × 10−3, where the CFD\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n13\n\nFig. 7. Comparison between physics-constrained DNN predictions and CFD solutions of idealized aneurysmal flows at three different viscosity\nsamples: (left) ν = 6.4 × 10−4, (middle) ν = 1.85 × 10−3, (right) ν = 2.14 × 10−3. (a) Streamwise velocity component u (b) spanwise\nvelocity component v (c) centerline wall shear profile τc.\n\nbenchmarks are plotted for comparison. In addition to the flow velocity and pressure, here we also investigate the\nwall shear stress (WSS) τ , which has been demonstrated as a critical factor affecting the aneurysm initialization,\nprogression, and rupture [74]. It can be seen from Fig. 7 that the DNN-predicted flow QoIs are in a good agreement\nwith the CFD solutions. The decrease of the flow velocity and the minimum WSS of the vessel are accurately\ncaptured by the DNN surrogate.\n\nFig. 8 shows the performance for learning the geometry-varying solutions of the aneurysmal flow, where the\ntraining is conducted by sampling the geometric parameter A, ranging from −2 × 10−2 to 0. The predicted flow\nfields of three different geometry samples are presented, where the size of the aneurysm increases from left to right.\nThe flow decelerates through the expanded region of the vessel and the velocity at the center of the aneurysm is\nsignificantly reduced, in particular when the aneurysm becomes larger. It is observed from the contour comparisons\n(Fig. 8a and b), the DNN predictions agree with the CFD solutions pretty well. As for the WSS profile, its shape\nand magnitude vary as the geometry changes, which can be accurately captured by the DNN surrogate (Fig. 8c).\n\nThe uncertainty propagation using the trained DNN surrogates is then conducted.\nUncertainties in viscosity ν and geometric parameter A with a truncated Gaussian and Gaussian distributions\nare considered, and the QoIs are center velocity uc and the minimum WSS τc, which are important for aneurysmal\nflows. As shown in Fig. 7c, the WSS remains invariant due to viscosity perturbation with a fixed vessel geometry,\nhence the distribution of uc is studied for viscosity uncertainty propagation (shown in Fig. 9a). The PDF obtained by\nthe DNN surrogate almost coincides with the CFD-based benchmark. As for the geometry uncertainty propagation,\nthe center velocity uc is not an interesting quantity since it almost remains the same as geometry changes. Instead,\nwe investigate the propagated uncertainty in the minimum WSS τc, which is sensitive to geometry variation. It is\nobserved from Fig. 9b that the probabilistic distribution of τc propagated by the DNN is in a favorable agreement\nwith the CFD-based benchmark, though the peak of the density is slightly underestimated.\n\n\f14\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 8. Comparison between physics-constrained DNN predictions and CFD solutions of idealized aneurysmal flows of three different\naneurysm geometries: (left) A = −3 × 10−3, (middle) A = −1.2 × 10−2, (right) A = −2.2 × 10−2. (a) Streamwise velocity component u (b)\nspanwise velocity component v (c) centerline wall shear profile τc.\n\nFig. 9. Probability density of the (a) center velocity uc propagated from a truncated Gaussian distributed viscosity ν and (b) the minimum wall\nshear τc propagated from a normally distributed geometric parameter A based on the trained DNN surrogate, compared against CFD-based\nMC solutions.\n\n3.2.3. Summary of training and prediction performance\n\nThe training and testing performance on the vascular flow cases are summarized in Table 1, where the training\nloss is defined as the sum of L 2 norms of momentum and continuity equation residuals (see Eq. (10)) and test error\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n15\n\nTable 1\nSummary of learning/prediction performance of vascular flows with 20 parameter\ncollocation points in parameter spaces for training.\n\nStenosis\n\nViscosity\n8 × 10−5\n8.18 × 10−5\n7.14 × 10−8\n2.33 × 10−5\n\nGeometry\n1 × 10−3\n9.61 × 10−4\n1.76 × 10−6\n2.23 × 10−3\n\nAneurysm\n\nViscosity\n5.5 × 10−5\n9.2 × 10−5\n1.33 × 10−7\n5.81 × 10−6\n\nGeometry\n2.9 × 10−5\n1.38 × 10−4\n1.15 × 10−6\n2.09 × 10−5\n\nTraining loss\nTest error eu\nTest error ev\nTest error e p\n\nFig. 10. Total test errors v.s. different number of training collocation points. The total collocation points is the multiplication of parameter\npoint size (N p) by geometric collocation point size (Ng = 104).\n\nis defined as the normalized L 2 difference between DNN and CFD results, shown as,\n\neu =\n\ne p =\n\n∥uD N N − uC F D∥2\n∆ p\n∥ pD N N − pC F D∥2\n(∆ p)2\n\n(16a)\n\n(16b)\n\nwhere pressure drop ∆ p = 0.1 is used to normalize the errors as it has the same dimension as ρu2 and ρ is set\n1 in all cases. As a result, the prediction performance is evaluated by calculating these dimensionless mean square\nerrors. It can be seen from Table 1 that both the training loss and test errors are reasonably small for all cases after\ntraining, and the test error in u is more dominant compared to that in v. Among the four scenarios, the geometric\nvariation of the stenosis is the most challenging to learn since the training loss (i.e., equation residual) remains\nrelatively large and the test errors are one-order bigger than the other cases. This is consistent with the notable\ndiscrepancy in the DNN-predicted PDF observed in Fig. 6b.\n\nAll the cases presented above are trained on N p = 20 parameter collocation points. Namely, the equation\nresiduals are minimized on 20 collocation points uniformly sampled from the parameter space. It is necessary\nto check if the size N p of parameter collocation points is sufficiently large for the training. Therefore, we conduct\na parameter study using different amounts of parameter collocation points for training. The total test errors (sum of\ntest errors in u, v, and p) against different numbers of training collocation points for all four cases are presented\nin Fig. 10.\n\nAs expected, the errors generally decrease as the number of training collocation points increases. However, the\nerror decreasing rate is quite mild. The test errors remain approximately unchanged when N p > 20 for all cases\nwhile the training cost will increase as more collocation points are used, which justifies the sufficiency of the total\ncollocation points used in this work. Detailed results of training cost and testing errors for all cases are summarized\nby Tables 4 and 5 in Appendix B.\n\n\f16\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 11. Physics-constrained learning results for a stenotic flow (A = 5 × 10−3, ν = 1 × 10−3) with (b) hard BC constraint, compared to\nthose using soft BC constraints with (c) λ = 1, (d) λ = 10, (e) λ = 100, (f) λ = 1000.\n\nLastly, we briefly discuss the computational cost of the uncertainty propagation tasks presented above, which\nis determined by the forward model-evaluation time and number of MC samples. The online evaluation of the\ntrained DNN surrogate is very fast, and the cost of each forward DNN evaluation is less than 2 × 10−2 CPU\nseconds. In contrast, the CFD simulation even for simple cases (e.g., stenotic and aneurysmal flows with a 2D\nmesh of 104 quadrilateral grids) takes around 40 CPU seconds. Therefore, remarkable speedups can be expected\nconsidering many-query applications like uncertainty propagation, and this advantage can be considerable when a\nlarger number of forward evaluations or more complicated fluid systems are considered. Moreover, the FV/FE-based\nCFD simulations require mesh generation, which is often a manually-cumbersome and labor-intensive process,\nin particular for the flows with complex geometries and moving boundaries, e.g., patient-specific cardiovascular\nsimulations. Therefore, the mesh-free feature of the proposed method is preferable in these applications.\n\n4. Discussion\n\n4.1. Pitfall on using soft boundary enforcement\n\nThe initial and boundary conditions (IC/BC) can be imposed in the physics-constrained learning either as a soft\nor hard constraint. When no labeled data is used in training, a properly enforced IC/BC is crucial to ensure the\nuniqueness of the learned PDE solutions. Although we have demonstrated the effectiveness of the hard enforcement\napproach in Section 3, it is still interesting to investigate the performance of soft enforcement method [33] in the\npurely PDE-driven training. Hence, all the test flows are studied again using physics-constrained learning, where\nBCs are imposed in a soft manner (as Eq. (17)). Namely, the BCs are formulated as a boundary loss component\nLB, which is incorporated into the physics-based loss function L phy as,\n\nLc\n\nphy = L phy + λLB,\n\n(17)\nwhere λ is the penalty coefficient. For the circular pipe flow case, both the hard and soft BC constraints can lead\nto excellent learning and prediction performance (results not shown). However, when the radius varies along the\ntube as in the stenosis and aneurysm cases, the DNN with the soft BC enforcement does not perform well as\nthe no-slip BC of the vessel wall is poorly imposed especially near the bottleneck in Fig. 11. Consequently, the\nsolution to the flow field becomes inaccurate. For example, Fig. 11 shows the results for learning a stenotic flow\n(A = 5 × 10−3) using the soft constraint with different λ values, where the result with hard boundary constraints\nand CFD benchmark are plotted as well for comparison.\n\nIn contrast to the result with hard constraints (Fig. 11b), both the flow patterns and magnitudes predicted\nwith the soft boundary enforcement with different λ (Fig. 11c–f) are completely wrong compared to the CFD\nbenchmark (Fig. 11a). Moreover, the nonlinear behavior of the centerline pressure profile cannot be accurately\ncaptured (results not shown). The poor performance reflects the major drawbacks of the soft constraints in the\nphysics-driven training as mentioned above. First, unique PDE solutions are determined by the IC/BC, which cannot\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n17\n\nTable 2\nConverged loss with different penalty coefficient λ.\nλ = 10\nLoss\n2 × 10−4\n3 × 10−3\n\nBoundary condition\nx-Momentum equation\n\nλ = 1\n1 × 10−3\n1 × 10−4\n\nλ = 100\n7 × 10−6\n8 × 10−3\n\nλ = 1000\n1 × 10−7\n1 × 10−2\n\nFig. 12. Learning curves of different activation functions on a stenotic flow (A = 5 × 10−3, ν = 1 × 10−3) with (a) hard BC constraint, (b)\nsoft BC constraint with the penalty coefficient λ = 1.\n\nbe guaranteed by simply penalizing the boundary loss. Moreover, it is difficult to assign the relative weight (λ) for\ndifferent components in the loss function, and there could be “competing” effects between the equation loss and\nboundary loss, which makes the optimization difficult to converge. As shown in Table 2, by assigning a large weight\nfor the boundary loss (λ = 1000), the boundary condition can be well prescribed but the PDE residual remains a\nlarge value and cannot be further reduced. On the other hand, when the weight for the boundary is small (λ = 1),\nthe loss of x-momentum equation can be reduced to O(10−4) but the BC fails to be imposed. It is important to\nnote that none of these four λ leads to a physical stenotic flow pattern as shown in Fig. 11.\n\n4.2. Role of activation function in physics-constrained learning\n\nThe performance of DNN training is affected by the activation function to a large degree. The widely used\nactivation functions include ReLU, Sigmoid, Tanh, etc. [75]. However, these activation functions are not guaranteed\nto be optimized in terms of the convergence rate and accuracy. Recent studies [76,77] proposed to train an\nadaptive activation function as well as the neural network weights to achieve better convergence property. Notably,\nRamachandran et al. [76] introduced an adaptive activation function called Swish, which is defined as x·Sigmoid(βx)\nand β is a trainable parameter. Jagtap and Karniadakis [77] presented a new adaptive function defined as Tanh(nax),\nwhere a is an adaptive parameter to be learned and n is a scale factor that potentially speeds up the convergence.\nIn current work, the training process uses a Swish activation function with fixed β = 1. It is not clear how\nthe adaptivity of activation function can affect the convergence rate and accuracy. Furthermore, as we discussed\nand highlighted the necessity of the hard BC enforcement in the physics-constrained data-free learning, it is also\ninteresting to compare the relative importance of boundary condition enforcement and adaptive activation function\non model performance. Therefore, we test the effects of different activation functions with/without hard boundary\nconstraints in the stenosis case (A = 5 × 10−3, ν = 1 × 10−3), and the resulting learning curves are shown in\nFig. 12, where panels (a) and (b) show the convergence histories of different activation functions with and without\na hard BC enforcement, respectively.\n\nThe legend “Swish-β” refers to the Swish function with adaptive β, while the “Swish” means the Swish function\nwith a fixed β = 1. The results of adaptive Tanh activation function with different n [77] are plotted as well. It\n\n\f18\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 13. Performance of the proposed DNN for solving an inviscid Burgers equation, with a smooth initial solution turning into a shock at\nt = 0.5.\n\ncan be seen in Fig. 12a that the convergence rate of using adaptive Tanh functions is faster than that of the Swish\nactivation functions within the first 400 epochs, but the training loss of all cases finally converge to the same\norder. Moreover, the convergence curves almost overlap with each other for the same type of activation function\nwith different hyperparameters. When the BC is imposed in a soft manner (Fig. 12b), the solution can be quickly\ntrapped in a bad local minimum as we discussed above and the final training loss is over one-order larger than\nthat of the cases with hard BC constraints. Using different adaptive activation functions does not help to further\ndecrease the loss and all the convergence curves are overlapped. Moreover, the convergence history of each trainable\nhyperparameter (i.e., β in Swish function and a in Tanh function) is monitored (see Fig. 14 in Appendix C), and we\nfound that these hyperparameters are more likely to converge to the optimized values when the BCs are enforced in\na hard manner. To sum up, the way of imposing BCs is found to be more critical than the adaptivity of activation\nfunction in our cases in terms of accuracy in the physics-constrained data-free learning.\n\n4.3. Physics-constrained learning for discontinuous solution\n\nIn this work, we mainly focused on incompressible fluid problems relevant to hemodynamics applications,\nwhere the flow solutions are usually smooth and continuous. To better explore the performance of the proposed\nIC/BC-encoded physics-constrained learning, canonical problems with discontinuous solutions were studied as well.\nSpecifically, the proposed method was applied to capture the solutions of hyperbolic PDEs with discontinuities,\ne.g., inviscid Burger’s equation, which is known to be notoriously difficult for most traditional numerical methods,\nespecially the continuous Galerkin finite element methods. As an example, here we show the learning results for a\none-dimensional inviscid Burger’s equation,\n\n∂u\n∂t\n\n+ u\n\n∂u\n∂ x\n\n= 0,\n\nwith a smooth initial condition as,\n\nu0 =\n\n1\nπ sin(2π x), x ∈ [0, 1].\n\n(18)\n\n(19)\n\nThe analytical solution is available, where the shock starts to form at t = 0.5 (i.e., ∂u\n∂ x\n\n→ ∞ at x = 0.5) and\nbecomes very sharp at t = 1.0, as shown in Fig. 13. These features can be well captured by our PDE-constrained\nDNN, where the IC and periodic BC were encoded into the DNN structure in a hard manner. It can be observed\nthat even the sharpest discontinuity shown in Fig. 13c can be accurately predicted as the DNN approximation (blue\ndots) is almost overlapped with the exact solution (red line). The mean square test error for this case is 3.64 × 10−6,\nand the training loss can be reduced to O(10−6). The results demonstrate the good potential of physics-constrained\nlearning for solving PDEs even with discontinuous behaviors. However, the loss function formulation here can be\ninterpreted as a strong form of the PDE, and we expect that discontinuous Galerkin (DG) formulation in a weak\nform may help to further improve the learning performance for discontinuous problems, which is out of the scope\nof this work.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n19\n\nFig. 14. Convergence histories of trainable parameters (β for Swish and na for Tanh) of adaptive activation functions, where panels (a)–(e)\nare for cases with hard BC enforcement and panels (f)–(j) are for cases with soft BC enforcement. The legend a1, a2, a3 refer to three\ndifferent layers.\n\n4.4. Physics-constrained data-free learning vs. traditional data-driven learning\n\nWe have demonstrated that the solutions of parametric Navier–Stokes equations can be effectively learned by\nsolely minimizing the PDE residuals without using any simulation data. To better evaluate the advantages and\nlimitations of the physics-constrained learning, we also conducted a series of comparison studies between the\nphysics-constrained data-free DNN and traditional data-driven DNN in terms of learning efficiency and prediction\naccuracy. Namely, all the vascular flow cases discussed above are learned again in a purely data-driven way, where\nthe DNN architecture and hyperparameters remain the same and only the physics-based loss function (i.e., Eq. (10))\nis replaced with the data-based one (i.e., Eq. (5)). A number of CFD simulations with different input parameters are\nconducted and the simulated velocity and pressure fields are collected as the training data. The detailed comparisons\nfor stenotic and aneurysmal flows cases with varying viscosity and geometry are summarized by Tables 4 and\n5 in Appendix B. In general, the prediction results from purely data-driven learning are slightly more accurate\nthan those of physics-constrained learning, and the accuracy of both models improves as the parameter collocation\npoints increase. However, data-driven learning requires additional offline CFD simulations and this computational\noverhead can quickly grow as more training points are sampled from the parameter space. In this paper, since the\nCFD cases considered here are not costly to simulate (e.g., each CFD simulation takes about 40 CPU seconds), the\ncomputational overhead due to the offline data generation process is not significant. Nonetheless, the advantage of\nthe data-free feature in physics-constrained learning will become more notable when large-scale 3D flow problems\nare considered, where a single simulation run could be very expensive. It notes that when training data are ready for\nuse, the cost of data-driven training process is approximately similar to that of the physics-constrained training (in the\nsame order), though the latter is slightly slower due to the additional AD calculations for derivatives. To reduce the\ntraining cost in data-driven learning, one way is to reduce the spatial dimensionality by projecting the training data\n(i.e., velocity/pressure fields) onto the POD basis, and learning is performed on POD coefficients instead of spatial\ncollocation points. We also conducted POD-based data-driven learning and found that the learning performance\nwith the current shallow network structure is unsatisfactory (results are not shown here for conciseness).\n\nAdmittedly, the current form of physics-constrained DNN has its limitations, for example, the offline training\nprocess is still costly, the convergence cannot be guaranteed due to the non-convexity of DNN optimization, and\nscalability for high-dimensional complex problems is still challenging. The proposed PDE-constrained DNN is not\nexpected to replace the classical CFD (numerical) solvers, which have been developed for decades. However, the\ndevelopment of PDE-constrained DNN for surrogate modeling shows strong promise. Particularly, the proposed\nmethod is mesh-free and thus does not require arduous mesh generation labor and intensive domain expertise\nin numerical modeling, which is suitable for, e.g., rapidly testing ideas in the design phase. We expect that the\n\n\f20\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nTable 3\nSummary of test errors on ν-varying stenosis case for different DNN architectures, i.e., networks\nwith different number of hidden layers and neurons.\n\n1 Hidden layer\n\nNeurons per layer\n\nTest error eu\nTest error ev\n\nTest error e p\n\n2 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n3 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\n\nTest error e p\n\n4 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n5 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n10\n7.44 × 10−3\n9.31 × 10−6\n3.26 × 10−3\n\n10\n1.42 × 10−3\n1.05 × 10−6\n2.14 × 10−4\n\n10\n9.91 × 10−5\n9.71 × 10−8\n1.96 × 10−5\n\n10\n6.75 × 10−5\n5.33 × 10−8\n8.85 × 10−6\n\n10\n2.82 × 10−5\n6.12 × 10−8\n4.00 × 10−5\n\n20\n5.92 × 10−3\n6.83 × 10−6\n1.75 × 10−3\n\n20\n8.92 × 10−4\n6.23 × 10−7\n1.48 × 10−4\n\n20\n4.16 × 10−5\n3.72 × 10−8\n2.36 × 10−5\n\n20\n3.35 × 10−5\n2.23 × 10−8\n3.22 × 10−6\n\n20\n1.32 × 10−5\n3.04 × 10−8\n1.54 × 10−5\n\n30\n5.39 × 10−3\n5.7 × 10−6\n1.46 × 10−3\n\n30\n8.82 × 10−4\n6.72 × 10−7\n1.61 × 10−4\n\n30\n2.91 × 10−5\n3.52 × 10−8\n8.40 × 10−6\n\n30\n2.53 × 10−5\n1.86 × 10−8\n2.56 × 10−6\n\n30\n2.50 × 10−5\n7.55 × 10−8\n1.19 × 10−5\n\n40\n5.93 × 10−3\n7.08 × 10−6\n1.53 × 10−3\n\n40\n2.75 × 10−4\n2.91 × 10−7\n6.10 × 10−5\n\n40\n1.21 × 10−4\n3.67 × 10−8\n1.05 × 10−5\n\n40\n6.05 × 10−5\n7.32 × 10−8\n6.65 × 10−6\n\n40\n2.00 × 10−5\n2.86 × 10−8\n5.70 × 10−6\n\neffectiveness of surrogate model based on physics-constrained DNN will be significantly promoted along with the\npotential improvement of DNN training efficiency, e.g., a recent study has suggested that a novel photonic chip has\na potential to be used to train deep neural networks 10-million of times more efficiently than current CPUs/GPUs\ndo [78].\n\n5. Conclusion\n\nSurrogate modeling of fluid flows governed by the Navier–Stokes equations is significant for uncertainty\nquantification, optimization design, and inverse analysis in many engineering systems. As a universal function\napproximator, DNN is becoming a popular approach for surrogate modeling. However, training of a DNN often\nrequires large number of labeled data, which are usually not available for efficiently developing surrogates since each\ndata point requires an expensive CFD simulation. This paper presented a novel DNN surrogate for fluid simulations\nwithout using any labeled data (i.e., CFD simulation data). Specifically, a structured DNN architecture is devised\nto approximate the solutions of the parametric Navier–Stokes equations, where the initial/boundary conditions are\nsatisfied automatically. Instead of using any simulation data, the DNN is trained by solely minimizing the violation of\nthe mass and momentum conservation laws for fluid flows. Compared to the previous works of physics-constrained\nlearning, this paper focuses on modeling of fluid systems governed by parametric Navier–Stokes equations. The\nproposed methods were tested on three flow cases relevant to cardiovascular applications, i.e., circular pipe flow,\nstenotic flow, and aneurysmal flow. The DNNs with equation-based loss were trained to learn the flow fields with\nparameter variations in, e.g., viscosity and domain geometry. Uncertainties in these parameters are propagated\nthrough the trained DNN surrogate and the results are validated against the CFD benchmarks. The comparisons\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n21\n\nTable 4\nTraining and testing performance for stenosis case with varying viscosity/geometry.\n\nStenosis with varying viscosity (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n7598 s\n3.11 × 10−4\n4.13 × 10−4\n3.32 × 10−7\n6.08 × 10−5\n\n12977 s\n8 × 10−5\n8.18 × 10−5\n7.14 × 10−8\n2.33 × 10−5\n\n25988 s\n7.64 × 10−6\n6.02 × 10−5\n6.43 × 10−8\n9.82 × 10−6\n\n28543 s\n4.6 × 10−5\n3.29 × 10−5\n3.86 × 10−8\n6.87 × 10−6\n\n31996 s\n5.8 × 10−5\n7.27 × 10−5\n1.41 × 10−7\n1.41 × 10−5\n\nStenosis with varying viscosity (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6900 s\n400 s\n5 × 10−7\n1.49 × 10−5\n7.84 × 10−8\n6.42 × 10−7\n\n11967 s\n800 s\n5 × 10−7\n6.83 × 10−6\n2.37 × 10−7\n7.13 × 10−7\n\n19982 s\n1200 s\n1 × 10−7\n2.43 × 10−6\n8.84 × 10−8\n1.56 × 10−6\n\n22497 s\n1600 s\n1 × 10−7\n1.33 × 10−6\n3.09 × 10−8\n3.72 × 10−7\n\n30893 s\n2000 s\n1 × 10−7\n2.64 × 10−6\n3.01 × 10−8\n1.65 × 10−6\n\nStenosis with varying geometry (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6458 s\n5.0 × 10−4\n1.54 × 10−3\n2.93 × 10−6\n3.85 × 10−3\n\n11688 s\n1.0 × 10−3\n9.61 × 10−4\n1.76 × 10−6\n2.23 × 10−3\n\n18239 s\n3.0 × 10−4\n5.24 × 10−4\n1.26 × 10−6\n1.89 × 10−3\n\n24674 s\n7.0 × 10−4\n3.46 × 10−4\n7.43 × 10−7\n1.57 × 10−3\n\n42814 s\n1.0 × 10−4\n4.81 × 10−4\n9.53 × 10−7\n1.33 × 10−3\n\nStenosis with varying geometry (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n5498 s\n400 s\n3 × 10−7\n3.21 × 10−5\n2.11 × 10−7\n2.72 × 10−6\n\n11238 s\n800 s\n1 × 10−7\n2.18 × 10−5\n1.37 × 10−7\n2.88 × 10−7\n\n16285 s\n1200 s\n2 × 10−7\n2.4 × 10−5\n6.92 × 10−8\n1.22 × 10−6\n\n21848 s\n1600 s\n1 × 10−7\n2.74 × 10−5\n9.33 × 10−8\n3.41 × 10−7\n\n26396 s\n2000 s\n1 × 10−7\n3.12 × 10−5\n5.21 × 10−8\n6.40 × 10−7\n\nindicate the excellent agreement between the physics-constrained DNN surrogate models and CFD simulations.\nWithout using any labeled data in training, the DNN is able to accurately parameterize the velocity/pressure solutions\nwith varying viscosity and geometries, which can be used to efficiently propagate uncertainties with enormous MC\nsamples. Moreover, the performances of using hard and soft IC/BC enforcement approaches are compared and the\nissues of soft constraints in physics-constrained are discussed. We also investigated the influence of state-of-art\nadaptive activation functions and compared the present labeled-data-free learning approach with traditional data-\ndriven learning approach in terms of accuracy and efficiency. In summary, the results have demonstrated the merit\nof the proposed method and suggest a great promise in developing DNN for surrogate fluid models without the\nneed for CFD simulation data.\n\nAcknowledgments\n\nLS gratefully acknowledge partial funding of graduate fellowship from China Scholarship Council (CSC) in this\neffort. JXW would acknowledge support from the National Science Foundation (NSF contract CMMI-1934300) and\nDefense Advanced Research Projects Agency (DARPA) under the Physics of Artificial Intelligence (PAI) program\n(contract HR00111890034). The authors would like to thank Dr. Nicholas Zabaras and Mr. Yinhao Zhu for their\nhelpful discussions during this work. The authors also thank the anonymous reviewers for their comments and\nsuggestions, which helped improve the quality and clarity of the manuscript.\n\n\f22\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nTable 5\nTraining and testing performance for aneurysm case with varying viscosity/geometry.\n\nAneurysm with varying viscosity (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n5731 s\n9.71 × 10−5\n2.72 × 10−4\n2.22 × 10−7\n6.14 × 10−7\n\n14490 s\n5.50 × 10−5\n9.2 × 10−5\n1.33 × 10−7\n1.18 × 10−5\n\n25350 s\n1.07 × 10−5\n8.33 × 10−5\n1.31 × 10−7\n5.81 × 10−6\n\n24810 s\n2.33 × 10−5\n6.4 × 10−5\n5.96 × 10−8\n1.12 × 10−5\n\n31595 s\n2.22 × 10−5\n4.67 × 10−5\n7.46 × 10−8\n4.75 × 10−6\n\nAneurysm with varying viscosity (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6851 s\n400 s\n5 × 10−7\n1.76 × 10−4\n1.07 × 10−7\n6.14 × 10−7\n\n11861 s\n800 s\n5 × 10−7\n5.11 × 10−6\n6.31 × 10−8\n2.38 × 10−7\n\n24580 s\n1200 s\n1 × 10−7\n8.51 × 10−6\n1.56 × 10−7\n1.81 × 10−6\n\n24353 s\n1600 s\n1 × 10−7\n5.30 × 10−7\n6.95 × 10−8\n1.06 × 10−6\n\n31116 s\n2000 s\n1 × 10−7\n3.20 × 10−6\n7.42 × 10−8\n3.28 × 10−7\n\nAneurysm with varying geometry (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n7755 s\n8.8 × 10−5\n3.07 × 10−4\n1.67 × 10−6\n3.77 × 10−5\n\n12904 s\n2.9 × 10−5\n1.38 × 10−4\n1.15 × 10−6\n2.09 × 10−5\n\n24645 s\n1.8 × 10−5\n1.29 × 10−4\n1.01 × 10−6\n1.28 × 10−5\n\n26724 s\n7.8 × 10−6\n1.31 × 10−4\n9.95 × 10−7\n8.99 × 10−6\n\n29515 s\n1.9 × 10−5\n1.19 × 10−4\n9.06 × 10−7\n1.02 × 10−5\n\nAneurysm with varying geometry (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6322 s\n400 s\n3 × 10−7\n7.9 × 10−5\n8.77 × 10−7\n1.06 × 10−5\n\n11607 s\n800 s\n2 × 10−7\n8.63 × 10−5\n5.34 × 10−7\n8.94 × 10−6\n\n18582 s\n1200 s\n4 × 10−7\n7.74 × 10−5\n5.03 × 10−7\n8.96 × 10−6\n\n23309 s\n1600 s\n1 × 10−7\n8.69 × 10−5\n6.24 × 10−7\n9.24 × 10−6\n\n32124 s\n2000 s\n8 × 10−8\n6.73 × 10−5\n6.97 × 10−7\n1.26 × 10−5\n\nAppendix A. Learning performance of different network structures\n\nIn order to search for a network with the “simplest structure” that has the desired predictive accuracy for better\ntraining efficiency and generalizability, a series of tests on the ν-varying stenosis case were conducted using a group\nof DNNs with different number of hidden layers and neurons, from 1 hidden layer of 10 neurons per layer to 5\nhidden layers of 40 neurons per layer. The test errors for u, v and p of all the different networks are listed in Table 3.\nIn general, the test error decreases when the network becomes deeper (more hidden layers) and wider (more neurons\nper layer). We can see if the number of hidden layers is less than 3, adding one more layer can significantly reduce\nthe error by nearly an order of magnitude. However, the accuracy stays in the same order when the networks have\nmore than 3 layers. Furthermore, in the case of 3 hidden layers, there is no significant improvement by using more\nthan 20 hidden neurons per layer.\n\nAppendix B. Performance on learning idealized blood flows\n\nSee Tables 4 and 5.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n23\n\nAppendix C. Convergence of trainable parameters in activation functions\n\nFig. 14 shows the convergence histories of trainable parameters of different adaptive activation functions. The\ncomparison is also made between cases where the BCs are imposed in hard and soft manners. It can be found\nthat the parameters of activation function are more likely to converge with hard BC enforcement (panels (a)–(e))\ncompared to the cases with soft BC enforcement (panels (f)–(j)).\n\nReferences\n\n[1] J.D. Anderson, J. Wendt, Computational Fluid Dynamics, Vol. 206, Springer, 1995.\n[2] P. Benner, S. Gugercin, K. Willcox, A survey of projection-based model reduction methods for parametric dynamical systems, SIAM\n\nRev. 57 (4) (2015) 483–531.\n\n[3] P. Benner, A. Cohen, M. Ohlberger, K. Willcox, Model Reduction and Approximation: Theory and Algorithms, Vol. 15, SIAM, 2017.\n[4] T. Lassila, A. Manzoni, A. Quarteroni, G. Rozza, Model order reduction in fluid dynamics: challenges and perspectives, in: Reduced\n\nOrder Methods for Modeling and Computational Reduction, Springer, 2014, pp. 235–273.\n\n[5] C. Huang, K. Duraisamy, C. Merkle, Challenges in reduced order modeling of reacting flows, in: 2018 Joint Propulsion Conference,\n\n2018, p. 4675.\n\n[6] S. Chaturantabut, D.C. Sorensen, Nonlinear model reduction via discrete empirical interpolation, SIAM J. Sci. Comput. 32 (5) (2010)\n\n2737–2764.\n\n[7] B. Peherstorfer, Model reduction for transport-dominated problems via online adaptive bases and adaptive sampling, arXiv preprint ar\n\nXiv:1812.02094.\n\n[8] B. Peherstorfer, Z. Drmaˇc, S. Gugercin, Stabilizing discrete empirical interpolation via randomized and deterministic oversampling,\n\narXiv preprint arXiv:1808.10473.\n\n[9] W.-X. Ren, H.-B. Chen, Finite element model updating in structural dynamics by using the response surface method, Eng. Struct. 32\n\n(8) (2010) 2455–2465.\n\n[10] R.G. Regis, C.A. Shoemaker, A stochastic radial basis function method for the global optimization of expensive functions, INFORMS\n\nJ. Comput. 19 (4) (2007) 497–509.\n\n[11] M.C. Kennedy, A. O’Hagan, Predicting the output from a complex computer code when fast approximations are available, Biometrika\n\n87 (1) (2000) 1–13.\n\n[12] S. Atkinson, N. Zabaras, Structured bayesian gaussian process latent variable model: Applications to data-driven dimensionality reduction\n\nand high-dimensional inversion, J. Comput. Phys. 383 (2019) 166–195.\n\n[13] D. Xiu, G.E. Karniadakis, The Wiener–Askey polynomial chaos for stochastic differential equations, SIAM J. Sci. Comput. 24 (2)\n\n(2002) 619–644.\n\n[14] H.N. Najm, Uncertainty quantification and polynomial chaos techniques in computational fluid dynamics, Annu. Rev. Fluid Mech. 41\n\n(2009) 35–52.\n\n[15] X. Yang, X. Wan, L. Lin, L. Huan, A general framework for enhancing sparsity of generalized polynomial chaos expansions, Int. J.\n\nUncertain. Quantif. 9 (3) (2017) 221–243.\n\n[16] O. Le Maître, O.M. Knio, Spectral Methods for Uncertainty Quantification: With Applications to Computational Fluid Dynamics,\n\nSpringer Science & Business Media, 2010.\n\n[17] J.-X. Wang, C.J. Roy, H. Xiao, Propagation of input uncertainty in presence of model-form uncertainty: a multifidelity approach for\ncomputational fluid dynamics applications, ASCE-ASME J. Risk Uncertain. Eng. Syst. Part B: Mech. Eng. 4 (1) (2018) 011002.\n[18] Y. Zhu, N. Zabaras, Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification, J.\n\nComput. Phys. 366 (2018) 415–447.\n\n[19] R.K. Tripathy, I. Bilionis, Deep uq: Learning deep neural network surrogate models for high dimensional uncertainty quantification, J.\n\nComput. Phys. 375 (2018) 565–588.\n\n[20] S. Mo, N. Zabaras, X. Shi, J. Wu, Deep autoregressive neural networks for high-dimensional\n\ninverse problems in groundwater\n\ncontaminant source identification, Water Resour. Res. 55 (5) (2019) 3856–3881.\n\n[21] F. Scarselli, A.C. Tsoi, Universal approximation using feedforward neural networks: A survey of some existing methods, and some\n\nnew results, Neural Netw. 11 (1) (1998) 15–37.\n\n[22] M. Hutzenthaler, A. Jentzen, T. Kruse, T.A. Nguyen, P. von Wurstemberger, Overcoming the curse of dimensionality in the numerical\n\napproximation of semilinear parabolic partial differential equations, arXiv preprint arXiv:1807.01212.\n\n[23] P. Grohs, F. Hornung, A. Jentzen, P. Von Wurstemberger, A proof that artificial neural networks overcome the curse of dimensionality\n\nin the numerical approximation of black-scholes partial differential equations, arXiv preprint arXiv:1809.02362.\n\n[24] M. Hutzenthaler, A. Jentzen, T. Kruse, T.A. Nguyen, A proof that rectified deep neural networks overcome the curse of dimensionality\n\nin the numerical approximation of semilinear heat equations, arXiv preprint arXiv:1901.10854.\n\n[25] S. Lee, N. Baker, Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence, USDOE Office\n\nof Science (SC), United States, 2018.\n\n[26] G. Carleo, M. Troyer, Solving the quantum many-body problem with artificial neural networks, Science 355 (6325) (2017) 602–606.\n[27] J.-X. Wang, J.-L. Wu, H. Xiao, Physics-informed machine learning approach for reconstructing reynolds stress modeling discrepancies\n\nbased on dns data, Phys. Rev. Fluids 2 (3) (2017) 034603.\n\n[28] J. Ling, R. Jones, J. Templeton, Machine learning strategies for systems with invariance properties, J. Comput. Phys. 318 (2016) 22–35.\n\n\f24\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n[29] P.E. Shanahan, D. Trewartha, W. Detmold, Machine learning action parameters in lattice quantum chromodynamics, Phys. Rev. D 97\n\n(9) (2018) 094506.\n\n[30] R. King, O. Hennigh, A. Mohan, M. Chertkov, From deep to physics-informed learning of turbulence: Diagnostics, arXiv preprint ar\n\nXiv:1810.07785.\n\n[31] S.L. Brunton, J.N. Kutz, Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control, Cambridge\n\nUniversity Press, 2019.\n\n[32] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436.\n[33] M. Raissi, P. Perdikaris, G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse\n\nproblems involving nonlinear partial differential equations, J. Comput. Phys. 378 (2019) 686–707.\n\n[34] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., Deep\n\nspeech: Scaling up end-to-end speech recognition, arXiv preprint arXiv:1412.5567.\n\n[35] H. Lee, I.S. Kang, Neural algorithm for solving differential equations, J. Comput. Phys. 91 (1) (1990) 110–131.\n[36] I.E. Lagaris, A. Likas, D.I. Fotiadis, Artificial neural networks for solving ordinary and partial differential equations, IEEE Trans.\n\nNeural Netw. 9 (5) (1998) 987–1000.\n\n[37] I.E. Lagaris, A.C. Likas, D.G. Papageorgiou, Neural-network methods for boundary value problems with irregular boundaries, IEEE\n\nTrans. Neural Netw. 11 (5) (2000) 1041–1049.\n\n[38] M. Raissi, A. Yazdani, G.E. Karniadakis, Hidden fluid mechanics: A navier-stokes informed deep learning framework for assimilating\n\nflow visualization data, arXiv preprint arXiv:1808.04327.\n\n[39] M. Raissi, Z. Wang, M.S. Triantafyllou, G.E. Karniadakis, Deep learning of vortex-induced vibrations, J. Fluid Mech. 861 (2019)\n\n119–137.\n\n[40] A.M. Tartakovsky, C.O. Marrero, D. Tartakovsky, D. Barajas-Solano, Learning parameters and constitutive relationships with physics\n\ninformed deep neural networks, arXiv preprint arXiv:1808.03398.\n\n[41] X. Meng, G.E. Karniadakis, A composite neural network that learns from multi-fidelity data: Application to function approximation\n\nand inverse pde problems, arXiv preprint arXiv:1903.00104.\n\n[42] D. Zhang, L. Lu, L. Guo, G.E. Karniadakis, Quantifying total uncertainty in physics-informed neural networks for solving forward\n\nand inverse stochastic problems, arXiv preprint arXiv:1809.08327.\n\n[43] Y. Yang, P. Perdikaris, Adversarial uncertainty quantification in physics-informed neural networks, arXiv preprint arXiv:1811.04026.\n[44] R. Sharma, A.B. Farimani, J. Gomes, P. Eastman, V. Pande, Weakly-supervised deep learning of heat transport via physics informed\n\nloss, arXiv preprint arXiv:1807.11374.\n\n[45] M.A. Nabian, H. Meidani, Physics-informed regularization of deep neural networks, arXiv preprint arXiv:1810.05547.\n[46] K. Xu, E. Darve, The neural network approach to inverse problems in differential equations, arXiv preprint arXiv:1901.07758.\n[47] J.R. Holland, J.D. Baeder, K. Duraisamy, Towards integrated field inversion and machine learning with embedded neural networks for\n\nrans modeling, in: AIAA Scitech 2019 Forum, 2019, p. 1884.\n\n[48] R. Stewart, S. Ermon, Label-free supervision of neural networks with physics and domain knowledge, in: Thirty-First AAAI Conference\n\non Artificial Intelligence, 2017, pp. 2576–2582.\n\n[49] J. Sirignano, K. Spiliopoulos, Dgm: A deep learning algorithm for solving partial differential equations, J. Comput. Phys. 375 (2018)\n\n1339–1364.\n\n[50] J. Berg, K. Nyström, A unified deep artificial neural network approach to partial differential equations in complex geometries,\n\nNeurocomputing 317 (2018) 28–41.\n\n[51] E. Weinan, J. Han, A. Jentzen, Deep learning-based numerical methods for high-dimensional parabolic partial differential equations\n\nand backward stochastic differential equations, Commun. Math. Stat. 5 (4) (2017) 349–380.\n\n[52] C. Beck, E. Weinan, A. Jentzen, Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential\n\nequations and second-order backward stochastic differential equations, J. Nonlinear Sci. (2017) 1–57.\n\n[53] C. Beck, S. Becker, P. Grohs, N. Jaafari, A. Jentzen, Solving stochastic differential equations and Kolmogorov equations by means of\n\ndeep learning, arXiv preprint arXiv:1806.00421.\n\n[54] E. Weinan, B. Yu, The deep ritz method: A deep learning-based numerical algorithm for solving variational problems, Commun. Math.\n\nStat. 6 (1) (2018) 1–12.\n\n[55] J. Han, A. Jentzen, E. Weinan, Solving high-dimensional partial differential equations using deep learning, Proc. Natl. Acad. Sci. 115\n\n(34) (2018) 8505–8510.\n\n[56] M.A. Nabian, H. Meidani, A deep neural network surrogate for high-dimensional random partial differential equations, arXiv preprint\n\narXiv:1806.02957.\n\n[57] S. Karumuri, R. Tripathy, I. Bilionis, J. Panchal, Simulator-free solution of high-dimensional stochastic elliptic partial differential\n\nequations using deep neural networks, arXiv preprint arXiv:1902.05200.\n\n[58] Y. Zhu, N. Zabaras, P.-S. Koutsourelakis, P. Perdikaris, Physics-constrained deep learning for high-dimensional surrogate modeling and\n\nuncertainty quantification without labeled data, arXiv preprint arXiv:1901.06314.\n\n[59] A.G. Baydin, B.A. Pearlmutter, A.A. Radul, J.M. Siskind, Automatic differentiation in machine learning: a survey, J. Mach. Learn.\n\nRes. 18 (2018) 1–43.\n\n[60] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, A. Lerer, Automatic differentiation\n\nin PyTorch, in: NIPS Autodiff Workshop, 2017.\n\n[61] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: A system\nfor large-scale machine learning, in: 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 16, 2016,\npp. 265–283.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n25\n\n[62] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I.J. Goodfellow, A. Bergeron, N. Bouchard, Y. Bengio, Theano: new features and\nspeed improvements. deep learning and unsupervised feature learning, in: Neural Information Processing Systems Workshop (NIPS),\n2012, pp. 1–10.\n\n[63] R. Kleinberg, Y. Li, Y. Yuan, An alternative view: When does SGD escape local minima? arXiv preprint arXiv:1802.06175.\n[64] P. Márquez-Neila, M. Salzmann, P. Fua, Imposing hard constraints on deep networks: Promises and limitations, arXiv preprint arXiv:\n\n1706.02025.\n\n[65] P. Ramachandran, B. Zoph, Q.V. Le, Swish: a self-gated activation function, arXiv preprint arXiv:1710.05941.\n[66] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980.\n[67] X. Glorot, Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in: Proceedings of the International\n\nConference on Artificial Intelligence and Statistics, AISTATS’10, Society for Artificial Intelligence and Statistics, 2010.\n\n[68] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, in:\n\nProceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026–1034.\n\n[69] A. Blumer, A. Ehrenfeucht, D. Haussler, M.K. Warmuth, Occam’s razor, Inform. Process. Lett. 24 (6) (1987) 377–380.\n[70] H. Jasak, A. Jemcov, U. Kingdom, Openfoam: A c++ library for complex physics simulations, in: International Workshop on Coupled\n\nMethods in Numerical Dynamics, IUC, 2007, pp. 1–20.\n\n[71] S. Berger, L.-D. Jou, Flows in stenotic vessels, Annu. Rev. Fluid Mech. 32 (1) (2000) 347–382.\n[72] J.L. Brisman, J.K. Song, D.W. Newell, Cerebral aneurysms, New Engl. J. Med. 355 (9) (2006) 928–939.\n[73] J.R. Cebral, F. Mut, J. Weir, C.M. Putman, Association of hemodynamic characteristics and cerebral aneurysm rupture, Am. J.\n\nNeuroradiol. 32 (2) (2011) 264–270.\n\n[74] N. Chalouhi, B.L. Hoh, D. Hasan, Review of cerebral aneurysm formation, growth, and rupture, Stroke 44 (12) (2013) 3613–3622.\n[75] A.L. Maas, A.Y. Hannun, A.Y. Ng, Rectifier nonlinearities improve neural network acoustic models, in: Proceedings of the 30th Annual\n\nInternational Conference on Machine Learning, Vol. 30, 2013, p. 3.\n\n[76] P. Ramachandran, B. Zoph, Q.V. Le, Searching for activation functions, arXiv preprint arXiv:1710.05941.\n[77] A.D. Jagtap, G.E. Karniadakis, Adaptive activation functions accelerate convergence in deep and physics-informed neural networks,\n\narXiv preprint arXiv:1906.01170.\n\n[78] R. Hamerly, L. Bernstein, A. Sludds, M. Soljaˇci´c, D. Englund, Large-scale optical neural networks based on photoelectric multiplication,\n\nPhys. Rev. X 9 (2) (2019) 021032.\n\n"
    }
  ],
  "structuredContent": {
    "result": "# Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data\n\n**Type:** journalArticle\n\n**Item Key:** 2P33W7SZ\n\n**Date:** 04/2020\n\n**Authors:** Sun, Luning; Gao, Han; Pan, Shaowu; Wang, Jian-Xun\n\n**Journal:** Computer Methods in Applied Mechanics and Engineering, Volume 361, Pages 112732\n\n**DOI:** 10.1016/j.cma.2019.112732\n\n**URL:** https://linkinghub.elsevier.com/retrieve/pii/S004578251930622X\n\n\n\n## Extra\n\nTLDR: A physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data is provided, and results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\n\n\n\n## Abstract\n\nNumerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics and sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time applications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty quantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL) has shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality. However, the off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and interpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient in most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical simulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural network (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential equations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments are conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties in fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\n\n**Notes/Attachments:** 1\n\n---\n\n## Full Text\n\nAvailable online at www.sciencedirect.com\nScienceDirect\n\nComput. Methods Appl. Mech. Engrg. 361 (2020) 112732\n\nwww.elsevier.com/locate/cma\n\nSurrogate modeling for fluid flows based on physics-constrained deep\nlearning without simulation data\nLuning Suna,b, Han Gaoa,b, Shaowu Panc, Jian-Xun Wanga,b,∗\na Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, United States of America\nb Center for Informatics and Computational Science, University of Notre Dame, Notre Dame, IN, United States of America\nc Department of Aerospace Engineering, University of Michigan, Ann Arbor, MI, United States of America\n\nReceived 18 July 2019; received in revised form 4 November 2019; accepted 4 November 2019\nAvailable online 21 November 2019\n\nAbstract\n\nNumerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the\ngoverning equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics\nand sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time\napplications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty\nquantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL)\nhas shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality.\nthe off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and\nHowever,\ninterpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient\nin most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical\nsimulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL\napproach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural\nnetwork (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential\nequations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments\nare conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties\nin fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and\nforward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.\nc⃝ 2019 Elsevier B.V. All rights reserved.\n\nKeywords: Physics-informed machine learning; Label-free; Neural networks; Uncertainty quantification; Cardiovascular flows; Navier-Stokes\n\n1. Introduction\n\nComplex fluids are ubiquitous in natural and industrial processes, and accurately simulating the fluid flows is\nindispensable in many disciplines, e.g., aerospace, civil, and biomedical engineering. A fluid system is typically\ngoverned by the Navier–Stokes equations, which is a highly nonlinear partial differential equation (PDE) system.\n\n∗ Corresponding author at: Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, United\n\nStates of America.\n\nE-mail address:\n\njwang33@nd.edu (J.-X. Wang).\n\nhttps://doi.org/10.1016/j.cma.2019.112732\n0045-7825/ c⃝ 2019 Elsevier B.V. All rights reserved.\n\n\f2\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nNumerical simulation on fluid dynamics problems primarily relies on solving the PDE systems in a discretized\nform using, e.g., finite difference (FD), finite volume (FV), or finite element (FE) methods, which is known\nas the computational fluid dynamics (CFD) approach [1]. However, CFD simulations are often computationally\ncumbersome, especially for the flows with turbulence and complex geometries. Moreover, mesh generation also\nusually incurs a huge burden, in particular when moving boundary or large geometric variation is considered. The\nconsiderable computational expense greatly limits the use of principled CFD model to real-time predictions and\nmany-query analysis, which are highly relevant to many scientific problems and real-life applications, e.g., timely\nclinical diagnosis and surgery planning for cardiovascular diseases, optimization design of aircraft with large\nparameter variations, and forward/inverse uncertainty quantification (UQ) in high-consequence systems. As an\nis a computationally feasible way to tackle the aforementioned\nalternative, a cost-effective surrogate model\nchallenges.\n\nA surrogate model only approximates the input–output relation of a system, which can be evaluated efficiently.\nNamely, given input parameters, e.g., initial/boundary/operational conditions, the quantities of interest (QoIs), such\nas velocity, pressure, shear stress, and their integrals can be obtained rapidly without conducting the principled CFD\nsimulations. The existing surrogate modeling approaches can be roughly categorized into two classes: projection-\nbased reduced order models (ROMs) and data-fit models [2]. In projection-based ROMs, a reduced basis is extracted\nfrom the simulation data using an unsupervised learning technique, e.g., proper orthogonal decomposition (POD,\nalso known as principal component analysis) [3], and the full-order PDE operator is projected onto the subspace\nspanned by the reduced basis. As a result, the degrees of freedom of the system can be significantly reduced,\nand meanwhile, the underlying structure of the full-order model can be retained to a certain extent. Although\nholding some promises, the current projection-based ROM techniques have had limited impact on complex fluid\ndynamic problems mainly because of the stability and robustness issues [4,5]. Moreover, projection-based ROMs\nare highly code-intrusive and their speedup potential is limited when strong nonlinearity exists [6] though several\nremedies such as sparse sampling [7,8] exist. Another way to enable rapid simulations is to build a data-fit model,\nwhere a response surface of the system is learned from the simulation data in a supervised manner. Namely, a\ndeterministic or probabilistic input–output mapping is constructed using, e.g., polynomial basis functions [9], radial\nbasis functions [10], Gaussian process (GP) [11,12], and stochastic polynomial chaos expansion (PCE) [13–15],\namong others. All these models are built upon the CFD solutions of selected collocation points in parameter\nspace without the need to modify the codes of the CFD solver. Because of the non-intrusive feature and ease of\nimplementation, data-fit surrogates have been used for a wide range of forward and inverse uncertainty quantification\n(UQ) problems in fluid dynamics [14,16,17]. However, traditional data-fit models have a hard time handling\nthe problems with strong nonlinearities and high dimensionality. Deep learning (DL), in particular, the deep\nneural network (DNN) has become a popular surrogate modeling approach and has shown great potential to deal\nwith high-dimensional nonlinear UQ problems [18–20]. It has been shown that DNN as a universal function\napproximator [21] can overcome the curse of dimensionality in certain problems [22–24]. In broader scientific\ncomputing and physical modeling communities, machine learning (ML) has been receiving a lot of attentions\n[25–31]. However, the tremendous success of DL in the computer science, witnessed in areas of computer vision\nand image recognition [32], can be mainly attributed to the availability of large-scale labeled data (i.e., “big data”)\nand the interpolatory nature of their problems. Unfortunately, labeled data for surrogate modeling of fluid systems\nare often sparse and could be noisy, since they are obtained from either principled CFD simulations or experimental\nobservations, both of which are expensive to obtain. Therefore, in such “small data” regimes [33], the true power\nof DL cannot be fully exploited by naively using the off-the-shelf DL model in the computer science community\nas an end-to-end fashion [34] for a data-fit of surrogate modeling.\n\nIn conventional ML problems, the mechanism behind the system is usually unknown and thus can only be learned\nfrom the labeled data. In contrast, for modeling a physical system, the governing equations are usually known a\npriori but are difficult to solve efficiently. Instead of learning solely from the labeled data, e.g., solution of the\nstates on certain points in parameter space, the known governing equations can be utilized to constrain (or even\ndrive) the learning to compensate for the insufficiency of the data. Specifically, the training (optimization) of a DNN\ncan be driven by minimizing the residual of the governing equations constructed by the DNN ansatz. This idea of\nphysics-constrained learning is not new and was proposed back in the late ’90s in the context of solving classic\ndifferential equations [35–37]. However, limited by the NN techniques and computational power at that time, this\nseminal work did not have a big impact. Recently, this idea has been revived because of the recent advances in\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n3\n\ndeep learning [32] combined with ever-increasing computational resources. Notably, the physics-informed neural\nnetwork (PINN) proposed by Raissi et al. was used to solve a number of deterministic one-dimensional (1D) PDEs\nsuch as viscous Burger’s equation, and two/three-dimensional (2D/3D) PDE-constrained inverse problems with a\nmoderate amount of labeled data [33,38,39], e.g., measurements of the velocity field. A similar approach is also\napplied to learn the constitutive relationship in a Darcy flow [40]. The PINN approach has been recently extended\nto assimilate multi-fidelity training data [41], and its UQ analyses have been explored based on arbitrary polynomial\nchaos [42] and adversarial inference [43]. Similar ideas of using physical constraints to regularize the DNN training\nhave also been investigated in [44–47]. In the aforementioned works, a moderate amount of labeled data either\nfrom simulations or experimental measurements are still needed for obtaining an approximation to the solution of\nthe PDEs. In fact, if the initial and boundary conditions are well imposed thus the corresponding PDE problem\nis well-defined, in principle, the unique solution should be captured by the DNN via PDE-constrained learning\nwithout any labeled data. This is one of the main motivations of current work. Note that recently there have been\nseveral works on the concept of data-free DNNs, e.g., for solving a handful of computer vision problems [48],\ndeterministic PDEs [33,49,50], high-dimensional stochastic partial differential equations (SPDE), and backward\nstochastic differential equations (BSDE) [51–55].\n\nIn the context of surrogate modeling, Nabian and Meidani [56] and Karumuri et al. [57] applied the PDE-\nconstrained fully-connected neural network (FC-NN) for uncertainty propagation in steady heat equations. Zhu\net al. [58] proposed a PDE-constrained, label-free DNN surrogate model for UQ in an elliptic PDE using both the\nFC-NN and convolutional neural networks (CNN). Moreover, both the deterministic and probabilistic formulations\nof physics-constrained learning were studied. Their results have shown a significant potential of using the physics-\nconstrained DNN for surrogate modeling, where no labeled data are required during the training. Nonetheless, the\nsuccess has only been demonstrated in a number of canonical problems with regular (rectangular) geometries, and it\nremains unclear if the physics-constrained learning can handle realistic fluid systems governed by the Navier–Stokes\nequations in a parametric setting. Significant work is still needed to further explore the real-world problems for broad\nimpacts.\n\nThe objective of this paper is to develop a physics-constrained, data-free DNN for surrogate modeling of\nincompressible flows. A structured FC-NN is devised to approximate the solutions of parametric Navier–Stokes\nequations, where the initial/boundary conditions are enforced instead of being penalized together during training\nin previous works [33]. In addition, contrary to the existing data-driven surrogates, the training of our DNN is\nsolely driven by minimizing the residuals of the governing PDEs (i.e., conservation laws), where no expensive CFD\nsimulation data is used. The effectiveness and merits of the proposed method are demonstrated by investigating\na number of internal flows relevant to cardiovascular applications. The contributions of the current paper are\nsummarized as follows. First, this work explored the performance of label-free deep learning for parametric surrogate\nconstruction, while most existing PINN works focus on solving deterministic PDEs in a non-parametric setting [33].\nSecond, in this study a more challenging problem is tackled, where the system is governed by the full Navier–Stokes\nequations and with irregular geometries. Particularly, we demonstrated that the flow solutions to geometric variations\ncan be accurately captured by the proposed deep learning approach, showing good promise for rapid geometric\noptimization and uncertainty quantification. To the best of authors’ knowledge, this is the first attempt of using a\nsingle DNN structure to learn the solutions of Navier–Stokes equations in a parametric setting without relying on\nany labeled training data. Lastly, the boundary conditions are encoded into the DNN architecture in a hard manner,\nand the advantages of using “hard” boundary enforcement compared to the “soft” ones are demonstrated in data-\nfree scenarios. The current work aims to push forward the PDE-constrained deep learning framework towards more\nrealistic applications. The rest of the paper is organized as follows. The framework of structured FC-NN surrogate\nbased on the physics-constrained label-free training is introduced in Section 2. Numerical results of surrogate\nmodeling and uncertainty propagation on several vascular flows are presented in Section 3. The performance of\nsoft and hard boundary enforcement approaches, different adaptive activation functions, and data-free/data-driven\nlearning strategies are discussed in Section 4. Finally, conclusion is drawn in Section 5.\n\n\f4\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 1. A schematic diagram of the physics-constrained, data-free DL framework for surrogate modeling of fluid flows. A structured fully-\nconnected neural network (FC-NN) is developed with the boundary conditions encoded by construction. The network is trained by minimizing\nthe equation-based loss function and no CFD simulation data are needed.\n\n2. Methodology\n\n2.1. Overview\n\nMost low-speed flows, e.g., blood flows in large or medium sized vessels, can be described by the incompressible\n\nNavier–Stokes equations given as:\n∇ · u = 0,\n∂u\n∂t\n\nF (u, p) = 0 :=\n\n⎧\n⎨\n\n⎩\n\n+ (u · ∇)u +\n\nx, t ∈ Ω f,t , θ ∈ Rd ,\n\n1\nρ\n\n∇ p − ν∇2u + b f = 0,\n\nx, t ∈ Ω f,t , θ ∈ Rd\n\n(1)\n\nwhere t and x are time and space coordinates, respectively; Ω f,t ≜ Ω f × [0, T ]; θ is a d-dimensional parameter\nvector, including input and/or operational parameters such as fluid properties, inlets/outlets, and geometry of the\ndomain; both velocity u and pressure p are functions of time t, space x, variable parameters θ ; ρ and ν represent\nis the body force; Ω f ⊂ R3 denotes the fluid domain. The\ndensity and viscosity of the fluid, respectively; b f\nsolutions of velocity u and pressure p can be uniquely determined when suitable initial and boundary conditions\nare prescribed,\n\nI(x, p, u, θ ) = 0,\nB(t, x, p, u, θ ) = 0,\n\nx ∈ Ω f , t = 0, θ ∈ Rd ,\nx, t ∈ ∂Ω f × [0, T ], θ ∈ Rd ,\n\n(2a)\n\n(2b)\n\nwhere both I and B are general differential operators that define the initial and boundary conditions, respectively;\n∂Ω f denotes the boundary region. When a set of parameters θ is given, the flow fields, i.e., u(t, x) and p(t, x), can\nbe solved numerically by discretizing the Eqs. (1) and (2) using FD/FV/FE methods. However, this process involves\nmesh generation and iteratively solving large linear/nonlinear systems, which is usually time-consuming. Therefore,\npropagating the parameter uncertainty or inferring the unknown parameters through the FD/FV/FE solver becomes\nintractable when it comes to parametric problems, e.g., some parameters of θ are uncertain or unknown. Solving\nvarying-geometry problems is especially challenging since any change of the geometry requires regeneration of the\ncomputational meshes.\n\nTo enable fast predictions in terms of UQ and optimization applications, a deep neural network (DNN)\narchitecture is built to approximate the solutions of the Navier–Stokes equations in a parametric setting. The DNN-\nbased surrogate is expected to provide a rapid online prediction of the flow field with any given set of parameters θ\nafter the offline training. A schematic diagram of the proposed framework is shown in Fig. 1. A FC-NN is devised\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n5\n\nwith the input layer composed of time t, spatial coordinates x, and variable parameters θ . The raw outputs of the\nFC-NN (i.e., u and p) are used to construct the state variables (i.e., velocity ˆu and pressure ˆp) together with\ncontribution from the particular solution that encodes initial/boundary conditions. The FC-NN is trained by mini-\nmizing residuals of the Navier–Stokes equations and no data from CFD simulations are needed. Therefore, the DNN\npredictions are expected to conform to the conservation laws of fluid flows and satisfy the specified initial/boundary\nconditions. Note that the Navier–Stokes equations will not be solved with any numerical discretization. The details of\nthe physics-constrained training and boundary condition enforcement will be presented in the following subsections.\n\n2.2. Deep neural network and physics-constrained training\n\nNeural networks (NN) are a set of algorithms, inspired by the biological neural networks in brains, for\nclassification and regression tasks. There are various types of NNs with different neuron connection forms and\narchitectures, e.g., fully-connected neural networks (FC-NN), convolutional neural networks (CNN), and recurrent\nneural networks (RNN). In this work, the feedforward FC-NN is considered, where the neurons of adjacent layers\nare fully connected and outputs of each layer are fed forward as the inputs to the next layer. A FC-NN defines a\nmapping from the input layer z0 ∈ Rn0 to the output zL ∈ Rn L . The layers between the input and output layers are\ncalled hidden layers zl, where l = 1, . . . , L − 1. By convention, a neural network with more than one hidden layer\nis called a “deep” NN. Mathematically, two adjacent layers are connected as,\n\nzl = σl(WT\n\nl zl−1 + bl),\n\n(3)\nwhere Wl ∈ Rnl−1×nl and bl ∈ Rnl are the weight matrix and bias vector; the subscript l denotes the index\nof the layer; σl(·) is an activation function acting element-wise, for which a number of options can be chosen,\ne.g., sigmoids, rectified linear units (ReLU), and tanh functions. After training, the weights, bias, and activation\nfunction at each layer are determined, and the output prediction zL (i.e., velocity and pressure) can be rapidly\ncomputed from any given input vector z0 (i.e., coordinates and parameters) based on the Eq. (3). Since this\nfeedforward algorithm (Eq. (3)) only involves a few matrix multiplications, the computational cost for evaluating\nthe trained FC-NN can be neglected compared to that of a CFD simulation.\n\nTraditionally, to build a surrogate model for the CFD simulation of the solution f(t, x, θ ), one can simply consider\n\na black-box surrogate, e.g., FC-NN, or CNN [18], as zL (t, x, θ ; W, b), i.e.,\n\n(4)\n\nf(t, x, θ ) ≈ ˜f(t, x, θ ) ≜ zL (t, x, θ ; W, b),\np]⊺\n\nwhere f is the solution vector as f = [u\n, including velocity u and pressure p; W and b denote the weights and\nbiases of the entire network. Generally, training of a DNN is purely data-driven, and it consists of finding a set of\n(sub)optimal DNN parameters (W, b) such that the mismatch between the training data fd and the DNN predictions\n˜f is locally minimized. That is, one can formulate an optimization problem as,\n\nLdata(W, b) = \n\nfd (t, x, θ ) − zL (t, x, θ ; W, b)\nLdata(W, b),\n\nΩ f,t\n\n,\n\n(5a)\n\n(5b)\n\nW∗, b∗ = arg min\n\nW,b\n\nwhere the loss function Ldata(W, b) is named as “data-based loss”, and ∥·∥Ω f,t is L 2 norm over Ω f,t ; W∗, b∗ denote\na set of (sub)optimal NN weights and biases obtained from the optimization.\n\nHowever, as discussed above, this black-box surrogate modeling requires enormous training data fd , which is too\nexpensive to obtain from a large number of CFD simulations. Instead, following previous PINN framework [33], we\nconsider leveraging the governing PDEs in the loss function by minimizing the violation of the solution zL in terms\nof the known governing PDEs for fluid dynamics over a domain of interests without the needs of solving these\nequations for each parameter with traditional numerical methods. Specifically, only the residuals of the Navier–\nStokes equations are computed based on the FC-NN predictions and it corresponds to a constrained optimization\nas follows,\n\n∇ · ˜u\nL phy(W, b) = \nΩ f,t\n\n\n\nMass conservation\n\n+ ∥\n\n\n\n∂ ˜u\n∂t\n\n1\nρ\n\n+ ( ˜u · ∇) ˜u +\n\n∇ ˜p − ν∇2 ˜u + b f ∥Ω f,t\n\n\nMomentum conservation\n\n,\n\n(6)\n\n\f6\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nW∗ = arg min\n\nL phy(W),\n\n{\n\ns.t.\n\nW\n\nI(x, ˜p, ˜u, θ ) = 0,\nB(t, x, ˜p, ˜u, θ ) = 0,\n\nt = 0, in Ω f ,\non ∂Ω f,t ,\n\n(7)\n\nwhere the loss function L phy(W, b) here is named as “physics-based loss”; To construct the PDE residuals in the\nloss function, several first and/or second derivative terms of ˜u and ˜p with respect to time t and space coordinates x\nare required, which can be computed based on automatic differentiation (AD) [59]. AD is an accurate and efficient\nway to calculate derivatives in a computational graph, which has started to gain increasing attention in the machine\nlearning community. The general idea of AD is to use the chain rule to back-propagate derivatives from the output\nlayer to the inputs as the connection between each layer of a NN is analytically defined. Compared to numerical\ndifferentiation techniques, derivatives calculated from AD are much more accurate since they do not suffer from\ntruncation or round-off errors. Most modern deep learning frameworks such as PyTorch [60], TensorFlow [61], and\nTheano [62] have the AD implemented. To solve the optimization problem defined in Eq. (6), stochastic gradient\ndescent (SGD) algorithms are used, which are known to be a stochastic approximation of the gradient descent (GD)\noptimization. In SGD, only a subset of points are randomly sampled from the input space to calculate the direction\nof the gradient at each iteration. The SGD algorithms are known to work very well to escape bad local minima\nin the neural network training [63] under one point convexity property. Although the global minimum cannot be\nguaranteed for a non-convex optimization problem as defined in Eq. (6), an empirically good local minimum is\nusually found based on the SGD algorithms.\n\n2.3. Boundary condition enforcement\n\nIf the physics-based loss L phy becomes identically zero, the DNN predictions of velocity ˜u and pressure ˜p will\nexactly satisfy the Navier–Stokes equations (Eq. (1)). Therefore, penalizing the PDE residuals can regularize the\ndata-driven DNN solutions to be more physical. This idea is known as the physics-informed, weakly-supervised\ndeep learning [33,38,45]. To make the problem well-posed, proper initial and boundary conditions (IC/BC) are\nneeded and imposed as constraints (Eq. (7)) which are often treated in a “soft” manner by modifying the original\nloss function with penalty terms [33,64]. For example, the IC/BC can be imposed in a “soft” way by modifying\nEq. (6) as,\n\nLc\n\nphy(W, b, λi , λb) = L phy(W, b)\n\n\n\nEquation loss\n\n+ λi\n\n\nI(x, ˜p, ˜u, θ )\n\n\nInitial loss\n\nΩ f,t\n\n\n+ λb\n\n\nB(t, x, ˜p, ˜u, θ )\n\n\nBoundary loss\n\n∂Ω f,t\n\n\n,\n\n(8)\n\nwhere λi and λb are penalty coefficients. However, the soft IC/BC enforcement methods have several major\ndrawbacks: (1) there is no quantitative guarantee on how accurate the IC/BC being imposed and thus the solution\ncould be unsatisfactory; (2) the optimization performance can depend on the relative importance of each term, but\nhow to assign weight for each term can be difficult. Alternatively, we can impose the IC/BC in a “hard” manner,\nwhere a particular solution that solely satisfies the initial/boundary condition is added. Hence, the constraints on\nIC/BC are automatically fulfilled. A mixed enforcement on IC/BC is proposed in this work, where the Neumann\nand Dirichlet boundary conditions (BC) are treated separately: the Neumann BC are formulated into the equation\nloss, i.e., in a soft manner, while the IC and Dirichlet BC are encoded in a hard manner by constructing the DNN\nansatz ˆu and ˆp with a particular solution as follows,\n\nˆu(t, x, θ ; W, b) = u par (t, x, θ ) + D(t, x, θ ) ˜u(t, x, θ ; W, b),\nˆp(t, x, θ ; W, b) = p par (t, x, θ ) + D(t, x, θ ) ˜p(t, x, θ ; W, b),\n\n(9)\n\n= ub(x), p par (x, t)|x∈∂Ω f\n\nis a particular solution that just satisfies IC and BC: u par (x, 0) = u0(x), p par (x, 0) = p0(x) and\nwhere u par\nu par (x, t)|x∈∂Ω f\n= pb(x); D(t, x, θ ) is a globally defined smooth function from internal\npoints to the “boundary” in Ω f,t , i.e., a space–time sense. That is, D is zero on the boundary ∂Ω f × [0, T ] and\nΩ f × {0} while increases away from the boundary. For those problems where the IC/BC and the geometry of the\ndomain in Ω f,t is simple, the function D and particular solution can be written analytically. However, if the geometry\nis too complex to have an analytic form, e.g., a patient-specific artery, both the particular solution (u par , p par )\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n7\n\nand smooth distance function (D) can be parameterized by several pre-trained NNs similarly as [50]. These pre-\ntrained NNs can be designed to over-fit the boundary values, since they are only used to represent the particular\nsolutions or distance function as differentiable forms. Finally, the constrained optimization problem in Eq. (6) can\nbe reformulated as an unconstrained one, as shown in Eq. (10).\n\n∇ · ˆu\nphy(W, b) = \nLc\n\n\nMass conservation\nLc\n\nW∗, b∗ = arg min\n\nΩ f\n\n\nW,b\n\n∂ ˆu\n∂t\n\n\nphy(W, b),\n\n+ ∥\n\n+ ( ˆu · ∇) ˆu +\n\n1\n∇ ˆp − ν∇ 2 ˆu + b f ∥Ω f\nρ\n\n\nMomentum conservation\n\n(10a)\n\n(10b)\n\nNote that since the Neumann BCs were not studied numerically in this work, the loss term associated with Neumann\nBC is omitted in Eq. (10) and thus the total loss only contains the L 2 residual of governing equations.\n\n3. Numerical results\n\nA number of 2D vascular flows with idealized geometries, including circular pipe flows, stenotic flows, and\naneurysmal flows, are studied to evaluate the performance of the proposed data-free DL surrogate model in Eq. (10).\nForward propagation of the uncertainties in the fluid properties and domain geometry to the flow QoIs (e.g., velocity,\npressure, and wall shear stress) are investigated through the DNN surrogate model. In this study, only steady-state\nsolutions are considered for proof-of-concept, thus the constraint of initial condition can be neglected in Eq. (9).\n\nA composite FC-NN architecture is devised for the surrogate, which is composed of three sub-DNNs with an\nidentical structure of 3 hidden layers with 20 neurons per layer. The Swish activation function [65], defined as\nx · Sigmoid(βx) with a fixed hyperparameter β = 1, is employed in each layer except the last one, where a linear\nactivation function is used. The three sub-DNNs share the same input layer and separately predict three scalar\nstate variables, i.e., velocity u, v, and pressure p. All three sub-DNNs are trained simultaneously with a unified\nphysics-based loss function. To solve the unconstrained optimization problem defined in Eq. (10), we used the\nAdam optimizer [66], a robust variant of the SGD method, where the learning rate is adaptively changed based on\nthe estimates of the moments. The initial learning rate and mini-batch size are set as 1 × 10−3 and 50, respectively.\nBecause of the adaptivity feature of the Adam optimizer, the hyperparameters in the training are robust to some\nextent and require little tuning. Properly initializing the DNN parameters is also important. After comparing several\nwidely-used initialization schemes [67,68], we chose the He’s normal initializer [68], where initial weights are drawn\nfrom a truncated normal distribution. In general, a good choice of the DNN architecture, including the number of\nlayers, number of neurons per layer, activation function, and initialization schemes, is important to the learning\nperformance but is still determined by trial and error. A rule of thumb is to achieve the desired learning performance\nusing the “simplest” network structure, which is known as “Occam’s razor” [69]. Namely, a DNN structure with\nthe minimum number of layers and neurons still having the desired performance is preferable, because it usually\nenables efficient training and better generalizability. To search for such a structure, a series of tests were conducted.\nSpecifically, we studied the ν-varying stenosis case using a group of DNN structures with different number of hidden\nlayers and neurons, from the shallowest network with only 1 hidden layer of 10 neurons per layer to the largest one\nwith 5 hidden layers of 40 neurons per layer. By comparing the test errors of all cases (Table 5), we found the test\nerrors do not notably decrease with the DNN structures of more than 3 hidden layers and 20 neurons in each layer,\nwhich justified the current choice of the DNN structure. More details can be found in Appendix A. To demonstrate\nthe robustness of the physics-constrained learning, the architecture and hyperparameters remain the same for all the\ncases throughout the paper. Note that a comprehensive parameter study and architecture optimization of the DNN\nare out of the scope of the current work.\n\nThe composite FC-NN is implemented in the PyTorch platform [60]. As discussed in Section 2, only the\ncollocation points are required and they are uniformly sampled in the spatial x and parameter θ spaces. Alternatively,\none can choose a space-filling Latin hypercube sampling in the Ω f [33]. In this work, the PDE residuals are\nextensively evaluated on a large number of collocation points to ensure learning quality. For all test cases, the\ntraining of about 106 SGD iterations are performed on an NVIDIA GeForce GTX 1080 Ti Graphics Processing\nUnit (GPU) card, and the cost is approximately 3.5 h. Note that the offline training cost can be potentially reduced\nby optimizing the DNN hyperparameters. To validate the prediction performance of the trained DNN surrogates,\ncorresponding CFD simulations are also conducted using an open-source FV-based CFD solver, OpenFOAM [70].\nMesh convergence study is performed to ensure the solution accuracy. The code and datasets for this work will\nbecome available at https://github.com/Jianxun-Wang/LabelFree-DNN-Surrogate upon publication.\n\n\f8\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 2. (a) DNN predicted cross-section pipe flow velocity profiles u(y) of four different viscosity (ν) samples compared with the analytical\nsolution; (b) probability density function of the center velocity uc = u(y = 0) propagated from a truncated Gaussian distributed ν with mean\nν = 10−3 and variance of σν = 2.67 × 10−4, using the trained DNN surrogate compared with the analytical solution.\n\n3.1. Circular pipe flow\n\nFlow in a pipe/tube is very common in physiological systems, e.g., blood in arteries or air flow in trachea.\nThe pipe flow is often driven by the pressure difference between the two ends of a tube, or by the body force\nof gravity. In a cardiovascular system, the former one is more dominant since the blood flow is mainly governed\nby pressure drop due to the heart pumping. In general, simulating the fluid dynamics in a tube requires solving\nthe full Navier–Stokes equations numerically, but if the tube is straight and has a constant circular cross section,\nanalytical solution of the fully-developed steady-state flow is available, which is an ideal benchmark to validate the\nperformance of the proposed method. As a result, we first study the flow in a 2D circular pipe (also known as the\nPoiseuille flow).\n\nIn this case, the pressure inlet and outlet are used to drive the flow since we only focus on the fully-developed\nregime, and no-slip wall boundary is prescribed on the tube walls. The boundary conditions are encoded into the\nsurrogate model by constructing the DNN ansatz ˆu, ˆv, and ˆp based on Eq. (9). The no-slip condition of velocity\non the wall can be imposed by designing the ˆu, ˆv as,\n\nˆu =\n\n( d 2\n4\n\n)\n\n˜u,\n\n− y2\n\nˆv =\n\n( d 2\n4\n\n)\n\n˜v,\n\n− y2\n\nwhere y is the radial distance, d = 0.1 is the diameter of the tube, the raw DNN output is denoted by ˜u.\n\nThe pressure inlet pin = 0.1 and outlet pout = 0 are imposed by designing the ˆp as,\n\nˆp =\n\nx − xin\nxout − xin\n\npout +\n\nxout − x\nxout − xin\n\npin + (x − xin)(xout − x) ˜p,\n\n(11)\n\n(12)\n\nwhere xin and xout are coordinates of the two ends of the tube, and the raw DNN output is denoted by ˜p. All\nthree sub-DNNs are trained to capture the spatial flow fields with parameter variation in the fluid viscosity ν. Input\n(collocation) points in the parameter space of ν are uniformly sampled in the range 2 × 10−4 ≤ ν ≤ 1.9 × 10−3,\nwhere the corresponding Reynolds number (Re) are moderate (Re < 300). After training, both velocity and pressure\nfields can be obtained immediately by evaluating the trained DNN with any given input ν and a spatial grid on x.\nHence, the DNN surrogate can be utilized to propagate the uncertainty in viscosity ν based on the Monte Carlo\n(MC) simulation, where a large number of samples are drawn from the ν distribution and propagated to the QoIs\nvia the DNN surrogate. In the following test cases, 500 MC samples are used to compute desired statistics and\ndistributions.\n\nThe DNN surrogate results (shown in Fig. 2) are compared against the analytical solution, which is given by\n\nua =\n\n∆ p\n2νρ L\n\n( d 2\n4\n\n)\n\n,\n\n− y2\n\n(13)\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n9\n\nFig. 3. Learning performance in different ranges of Reynolds number Re. Solid lines indicate interpolation (testing within the training range)\nand dashed lines indicate extrapolation; ν20 and ν50 indicate that the number of collocation points in ν is 20 and 50, respectively. (For\ninterpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\n\nwhere y denotes the spanwise coordinate and ∆ p is the pressure difference. It can be observed from Fig. 2a that\nthe DNN-predicted velocity profiles (red dashed lines) of four different ν samples almost exactly agree with the\nanalytical solutions (blue solid lines), where the Reynolds numbers (Re) of the four cases are 283, 121, 33, and 3,\nrespectively. Actually, the trained DNN is able to accurately predict the pipe flow field with any given viscosity,\nwhere the Reynolds number is moderate. Fig. 2b shows the uncertainty of the center velocity uc propagated from\na truncated Gaussian distributed ν with mean of ν = 1 × 10−3 and variance of σν = 2.67 × 10−4 to guarantee the\nviscosity ν is always positive random variable. Namely, the truncated Gaussian distribution is defined with in the\nrange of (0, +∞), and the probability density function f is shown as,\n\nf (ν; ν, σν) =\n\n1\n\nσν N ( ν−ν\nσν )\n1 − φ(− ν\nσν )\n\n, ν > 0\n\n(14)\n\n∫ π\n0 e−t 2dt. It can be seen\nwhere N is the standard Gaussian density, and φ(x) = 1\n2\nthat the DNN-predicted probability density function (PDF) is almost identical to that of the analytical solutions,\ndemonstrating the excellent performance of the physics-constrained learning for uncertainty propagation.\n\n, with erf(z) = 2√\nπ\n\n1 + erf( x√\n2\n\n(\n\n)\n)\n\nTo comprehensively evaluate the learning performance, mean square errors of the DNN predictions are calculated\nand normalized by ∆ p. It is interesting to examine how well the predictions are across different Reynolds numbers\nRe in both the training range and the range of extrapolation. In current training setting, collocation points of ν are\nsampled from [2 × 10−4, 1.9 × 10−3], corresponding to Re ∈ [10, 300] as the training range. The trained DNN is\nalso tested on Re ∈ (300, 500], which is the range of extrapolation. Moreover, we also investigated the impact of\nRe on the learning performance by using a wider Re range for training. The mean square test error eu for each\ncase are plotted against the Reynolds number Re in Fig. 3, where solid lines indicate interpolation (testing within\nthe training range) and dashed line indicates extrapolation.\n\nIt is clear that the test error in the interpolation range (Re ∈ [10, 300], red solid) is relatively lower than that in\nthe extrapolation range (Re ∈ (300, 500], red dashed). If the training set includes Re ∈ (300, 500], the test error\ncan be largely reduced by nearly an order of magnitude (see the comparison of black solid line and red dashed\nline). This is expected since the success of deep learning largely relies on the interpolatory nature of the problem,\nand the extrapolation is always much more challenging. However, the generalization ability is not critical in the\nlabel-free learning framework since any collocation points can be freely drawn in the range of interest for training.\nMoreover, the effect of the total number of collocation points was studied and we found that the test error can\nbe further reduced if more collocation points are sampled during the training. This can be observed by comparing\nthe blue and black lines, which correspond to the cases trained with 50 and 20 collocation points of ν (or Re),\nrespectively. Lastly, it can be seen that the test error tends to grow as the Re increases, regardless of interpolation or\n\n\f10\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nextrapolation, which indicates that the flow solutions with higher Reynolds are more difficult to be captured by the\nDNN. It might be because steep gradients of flows with higher Re pose more challenges on learning. We refined\nourselves to the laminar flow regime, and higher-Re turbulent flows is out of the scope of this work.\n\n3.2. Blood flow with standardized geometry\n\nTwo types of canonical vascular flows, stenotic flow and aneurysmal flow, with standardized vessel geometries\nare studied. A stenotic flow refers to the flow through a vessel where there is narrowing and re-expansion of the\nvessel wall. This local restriction of the vessel is related to many cardiovascular diseases, e.g., arteriosclerosis,\nstroke, and heart attack [71]. The vascular flow within an aneurysm, which is the expansion of an artery due to\nthe weakness of vessel walls, is called aneurysmal flow. The rupture of an aneurysm may cause life-threatening\nconditions, e.g., subarachnoid hemorrhage (SAH) due to cerebral aneurysm rupture [72], and investigation of the\nhemodynamics can improve the diagnosis and fundamental understanding of aneurysm progression and rupture [73].\nWhereas realistic vascular geometries are usually irregular and complex, including sites of curvature, bifurcation\nand junctions, idealized stenosis and aneurysm models are studied here for proof-of-concept. Namely, both the\nstenotic and aneurysmal vessels are idealized as an axisymmetric tube with a varying cross-section radius, which\nis parameterized by the following function,\n\nR(x) = R0 − A\n\n1\n(2πσ 2)\n\n√\n\nexp(−\n\n(x − µ)2\n2σ 2\n\n)\n\n(15)\n\nwhere R0 is the radius at the inlet, which is set as 0.05, and the sign of A determines if the vessel is stenotic\nor aneurysmal. Namely, a positive and negative sign correspond to the stenosis and aneurysm, respectively. Three\ncontrol parameters A, µ, and σ define the shape of the stenotic (aneurysmal) vessel. The scale parameter A controls\nthe curvature along the tube, and a larger |A| leads to narrower stenosis (broader aneurysm). Parameter µ defines\nthe streamwise location of the minimum (maximum) radius of the stenosis (aneurysm), and σ affects the steepness\nof the geometric variation. In this study, the latter two parameters µ and σ are fixed as 0.5 and 0.1, respectively.\nOnly A is considered as a variable parameter to control the degree of the stenosis (aneurysm).\n\nSimilar to the pipe flow, the pressure inlet/outlet and no-slip wall boundary conditions are prescribed for\nboth stenosis and aneurysm cases. The boundary-encoded sub-DNNs are constructed to learn the parametric flow\nsolutions, where the wall BC is imposed using the geometric function D(x, y) = R(x)2 − y2. Contrary to the case\nstudied above, where only the viscosity variation is considered, the DNN surrogate is also trained to capture the\nvarying stenosis (aneurysm) geometry, which is known challenging for mesh-based CFD simulations. Specifically,\nthe solutions of varying viscosity are learned for fixed vascular geometries (A = 5 × 10−3 for stenosis and\nA = −5 × 10−3 for aneurysm) in the first place, and then the performance for capturing geometry variations\nis examined at a fixed viscosity (ν = 1 × 10−3). Moreover, the uncertainties from the flow viscosity and vessel\ngeometry are propagated to the QoIs through the trained DNN surrogate using MC sampling, and the results are\nvalidated by CFD-based MC simulations.\n\n3.2.1. Flow in idealized stenosis\n\nThe DNN is trained to parameterize the solutions of stenotic flows with varying viscosity, where collocation\n\npoints are sampled in ν space within the range of [5 × 10−4, 1 × 10−2] for physics-based training.\n\nFig. 4 shows the DNN-predicted flow fields of three different viscosity samples, i.e., ν = 6.4×10−4, 1.85×10−3,\nand 2.14×10−3, at moderate Reynolds numbers. The corresponding CFD simulations are performed for comparison.\nIt can be seen that the flow patterns of different ν are similar, where the fluid is accelerated streamwisely through the\nconverging region and slows down passing the diverging part of the tube. However, the velocity magnitude reduces\nas the viscosity increases (left to right columns). As shown in Fig. 4a and b, the DNN-predicted velocity contours\nof streamwise and spanwise components (u and v) agree with the CFD solutions very well, though the magnitude\nin the case with the smallest viscosity (ν = 6.4 × 10−4) is slightly underestimated. Moreover, the nonlinear pressure\ndrops can be accurately captured by the DNN surrogate as the profiles of centerline pressure from the DNN and\nCFD are almost identical (Fig. 4c).\n\nTo learn the flow solutions with varying geometry, the DNN is trained on uniformly sampled points within the\n\nrange of 0 ≤ A ≤ 1 × 10−2.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n11\n\nFig. 4. Comparison between physics-constrained DNN predictions and CFD solutions of idealized stenotic flows at three different viscosity\nsamples: (left) ν = 6.4 × 10−4, (middle) ν = 1.85 × 10−3, (right) ν = 2.14 × 10−3. (a) Streamwise velocity component u (b) spanwise\nvelocity component v (c) centerline pressure profile Pc.\n\nWe compare the flow fields predicted by the trained DNN against the CFD benchmarks, and the results of three\ndifferent samples of stenosis geometries, A = 2 × 10−3, 4 × 10−3, and 7 × 10−3, are shown in Fig. 5. From left to\nright, the degree of stenosis increases and thus the total flow rate is reduced due to the increased resistance (Fig. 5a\nand b). The pressure drop becomes more nonlinear as the stenotic vessel turns to be narrower (Fig. 5c). We can see\nthat the DNN predictions can capture these flow features and agree with the CFD benchmarks well. Admittedly,\nthe streamwise velocity of the flow with the narrowest stenotic vessel is slightly underestimated, and the centerline\npressure profiles predicted by the DNN and CFD has a small discrepancy. This might be because the increased\nnonlinearity due to the steep geometric variation poses a challenge on the learning.\n\nAfter the training, the DNN surrogates are used to rapidly propagate uncertainties in viscosity and vessel\ngeometry, and the effects on QoIs are investigated. Specifically, 500 MC samples are drawn from a truncated\nGaussian distribution of viscosity ν and a Gaussian distribution of geometric parameter A, which are propagated to\nthe center velocities uc (at x = 0.5, y = 0.0) through the DNN surrogates. The probability distributions of uc due\nto uncertain viscosity and vessel geometry are shown in Fig. 6a and b, respectively, where the propagated results\nthrough the CFD solver are also plotted for comparison.\n\nIt shows that the propagated distributions of uc are non-Gaussian in both cases, which is due to the strong\nnonlinearity of the Navier–Stokes operator. As expected,\nthe DNN-propagated uncertainties present a good\nagreement with the CFD-based benchmarks, especially in the case of viscosity uncertainty propagation (Fig. 6a),\nwhere the two PDF curves are almost overlapped with each other. As for the geometry uncertainty propagation,\nalthough the overall feature of the PDF is captured, the peak density is slightly underpredicted by the DNN. The\nreason behind this could be that the DNN surrogate tends to underestimate the velocity magnitude in particular for\na steep geometry variation.\n\n\f12\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 5. Comparison between physics-constrained DNN predictions and CFD solutions of idealized stenotic flows with three different stenosis\ngeometries: (left) A = 2 × 10−3, (middle) A = 4 × 10−3, (right) A = 7 × 10−3. (a) Streamwise velocity component u (b) spanwise velocity\ncomponent v (c) centerline pressure profile Pc.\n\nFig. 6. Probability density of the center velocity uc propagated from (a) a truncated Gaussian distributed viscosity ν and (b) a normally\ndistributed geometric parameter A based on the trained DNN surrogate, compared against CFD-based MC solutions.\n\n3.2.2. Flow in idealized aneurysm\n\nWe first learn the aneurysmal flows with varying viscosity, where the geometry of the aneurysm is fixed\n\n(A = −5 × 10−3).\n\nTraining is conducted by sampling the viscosity points ranging from 5 × 10−4 to 1 × 10−2. Fig. 7 shows the\nDNN-predicted flow fields of three viscosity samples, i.e, ν = 6.4 × 10−4, 1.85 × 10−3, 2.14 × 10−3, where the CFD\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n13\n\nFig. 7. Comparison between physics-constrained DNN predictions and CFD solutions of idealized aneurysmal flows at three different viscosity\nsamples: (left) ν = 6.4 × 10−4, (middle) ν = 1.85 × 10−3, (right) ν = 2.14 × 10−3. (a) Streamwise velocity component u (b) spanwise\nvelocity component v (c) centerline wall shear profile τc.\n\nbenchmarks are plotted for comparison. In addition to the flow velocity and pressure, here we also investigate the\nwall shear stress (WSS) τ , which has been demonstrated as a critical factor affecting the aneurysm initialization,\nprogression, and rupture [74]. It can be seen from Fig. 7 that the DNN-predicted flow QoIs are in a good agreement\nwith the CFD solutions. The decrease of the flow velocity and the minimum WSS of the vessel are accurately\ncaptured by the DNN surrogate.\n\nFig. 8 shows the performance for learning the geometry-varying solutions of the aneurysmal flow, where the\ntraining is conducted by sampling the geometric parameter A, ranging from −2 × 10−2 to 0. The predicted flow\nfields of three different geometry samples are presented, where the size of the aneurysm increases from left to right.\nThe flow decelerates through the expanded region of the vessel and the velocity at the center of the aneurysm is\nsignificantly reduced, in particular when the aneurysm becomes larger. It is observed from the contour comparisons\n(Fig. 8a and b), the DNN predictions agree with the CFD solutions pretty well. As for the WSS profile, its shape\nand magnitude vary as the geometry changes, which can be accurately captured by the DNN surrogate (Fig. 8c).\n\nThe uncertainty propagation using the trained DNN surrogates is then conducted.\nUncertainties in viscosity ν and geometric parameter A with a truncated Gaussian and Gaussian distributions\nare considered, and the QoIs are center velocity uc and the minimum WSS τc, which are important for aneurysmal\nflows. As shown in Fig. 7c, the WSS remains invariant due to viscosity perturbation with a fixed vessel geometry,\nhence the distribution of uc is studied for viscosity uncertainty propagation (shown in Fig. 9a). The PDF obtained by\nthe DNN surrogate almost coincides with the CFD-based benchmark. As for the geometry uncertainty propagation,\nthe center velocity uc is not an interesting quantity since it almost remains the same as geometry changes. Instead,\nwe investigate the propagated uncertainty in the minimum WSS τc, which is sensitive to geometry variation. It is\nobserved from Fig. 9b that the probabilistic distribution of τc propagated by the DNN is in a favorable agreement\nwith the CFD-based benchmark, though the peak of the density is slightly underestimated.\n\n\f14\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 8. Comparison between physics-constrained DNN predictions and CFD solutions of idealized aneurysmal flows of three different\naneurysm geometries: (left) A = −3 × 10−3, (middle) A = −1.2 × 10−2, (right) A = −2.2 × 10−2. (a) Streamwise velocity component u (b)\nspanwise velocity component v (c) centerline wall shear profile τc.\n\nFig. 9. Probability density of the (a) center velocity uc propagated from a truncated Gaussian distributed viscosity ν and (b) the minimum wall\nshear τc propagated from a normally distributed geometric parameter A based on the trained DNN surrogate, compared against CFD-based\nMC solutions.\n\n3.2.3. Summary of training and prediction performance\n\nThe training and testing performance on the vascular flow cases are summarized in Table 1, where the training\nloss is defined as the sum of L 2 norms of momentum and continuity equation residuals (see Eq. (10)) and test error\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n15\n\nTable 1\nSummary of learning/prediction performance of vascular flows with 20 parameter\ncollocation points in parameter spaces for training.\n\nStenosis\n\nViscosity\n8 × 10−5\n8.18 × 10−5\n7.14 × 10−8\n2.33 × 10−5\n\nGeometry\n1 × 10−3\n9.61 × 10−4\n1.76 × 10−6\n2.23 × 10−3\n\nAneurysm\n\nViscosity\n5.5 × 10−5\n9.2 × 10−5\n1.33 × 10−7\n5.81 × 10−6\n\nGeometry\n2.9 × 10−5\n1.38 × 10−4\n1.15 × 10−6\n2.09 × 10−5\n\nTraining loss\nTest error eu\nTest error ev\nTest error e p\n\nFig. 10. Total test errors v.s. different number of training collocation points. The total collocation points is the multiplication of parameter\npoint size (N p) by geometric collocation point size (Ng = 104).\n\nis defined as the normalized L 2 difference between DNN and CFD results, shown as,\n\neu =\n\ne p =\n\n∥uD N N − uC F D∥2\n∆ p\n∥ pD N N − pC F D∥2\n(∆ p)2\n\n(16a)\n\n(16b)\n\nwhere pressure drop ∆ p = 0.1 is used to normalize the errors as it has the same dimension as ρu2 and ρ is set\n1 in all cases. As a result, the prediction performance is evaluated by calculating these dimensionless mean square\nerrors. It can be seen from Table 1 that both the training loss and test errors are reasonably small for all cases after\ntraining, and the test error in u is more dominant compared to that in v. Among the four scenarios, the geometric\nvariation of the stenosis is the most challenging to learn since the training loss (i.e., equation residual) remains\nrelatively large and the test errors are one-order bigger than the other cases. This is consistent with the notable\ndiscrepancy in the DNN-predicted PDF observed in Fig. 6b.\n\nAll the cases presented above are trained on N p = 20 parameter collocation points. Namely, the equation\nresiduals are minimized on 20 collocation points uniformly sampled from the parameter space. It is necessary\nto check if the size N p of parameter collocation points is sufficiently large for the training. Therefore, we conduct\na parameter study using different amounts of parameter collocation points for training. The total test errors (sum of\ntest errors in u, v, and p) against different numbers of training collocation points for all four cases are presented\nin Fig. 10.\n\nAs expected, the errors generally decrease as the number of training collocation points increases. However, the\nerror decreasing rate is quite mild. The test errors remain approximately unchanged when N p > 20 for all cases\nwhile the training cost will increase as more collocation points are used, which justifies the sufficiency of the total\ncollocation points used in this work. Detailed results of training cost and testing errors for all cases are summarized\nby Tables 4 and 5 in Appendix B.\n\n\f16\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 11. Physics-constrained learning results for a stenotic flow (A = 5 × 10−3, ν = 1 × 10−3) with (b) hard BC constraint, compared to\nthose using soft BC constraints with (c) λ = 1, (d) λ = 10, (e) λ = 100, (f) λ = 1000.\n\nLastly, we briefly discuss the computational cost of the uncertainty propagation tasks presented above, which\nis determined by the forward model-evaluation time and number of MC samples. The online evaluation of the\ntrained DNN surrogate is very fast, and the cost of each forward DNN evaluation is less than 2 × 10−2 CPU\nseconds. In contrast, the CFD simulation even for simple cases (e.g., stenotic and aneurysmal flows with a 2D\nmesh of 104 quadrilateral grids) takes around 40 CPU seconds. Therefore, remarkable speedups can be expected\nconsidering many-query applications like uncertainty propagation, and this advantage can be considerable when a\nlarger number of forward evaluations or more complicated fluid systems are considered. Moreover, the FV/FE-based\nCFD simulations require mesh generation, which is often a manually-cumbersome and labor-intensive process,\nin particular for the flows with complex geometries and moving boundaries, e.g., patient-specific cardiovascular\nsimulations. Therefore, the mesh-free feature of the proposed method is preferable in these applications.\n\n4. Discussion\n\n4.1. Pitfall on using soft boundary enforcement\n\nThe initial and boundary conditions (IC/BC) can be imposed in the physics-constrained learning either as a soft\nor hard constraint. When no labeled data is used in training, a properly enforced IC/BC is crucial to ensure the\nuniqueness of the learned PDE solutions. Although we have demonstrated the effectiveness of the hard enforcement\napproach in Section 3, it is still interesting to investigate the performance of soft enforcement method [33] in the\npurely PDE-driven training. Hence, all the test flows are studied again using physics-constrained learning, where\nBCs are imposed in a soft manner (as Eq. (17)). Namely, the BCs are formulated as a boundary loss component\nLB, which is incorporated into the physics-based loss function L phy as,\n\nLc\n\nphy = L phy + λLB,\n\n(17)\nwhere λ is the penalty coefficient. For the circular pipe flow case, both the hard and soft BC constraints can lead\nto excellent learning and prediction performance (results not shown). However, when the radius varies along the\ntube as in the stenosis and aneurysm cases, the DNN with the soft BC enforcement does not perform well as\nthe no-slip BC of the vessel wall is poorly imposed especially near the bottleneck in Fig. 11. Consequently, the\nsolution to the flow field becomes inaccurate. For example, Fig. 11 shows the results for learning a stenotic flow\n(A = 5 × 10−3) using the soft constraint with different λ values, where the result with hard boundary constraints\nand CFD benchmark are plotted as well for comparison.\n\nIn contrast to the result with hard constraints (Fig. 11b), both the flow patterns and magnitudes predicted\nwith the soft boundary enforcement with different λ (Fig. 11c–f) are completely wrong compared to the CFD\nbenchmark (Fig. 11a). Moreover, the nonlinear behavior of the centerline pressure profile cannot be accurately\ncaptured (results not shown). The poor performance reflects the major drawbacks of the soft constraints in the\nphysics-driven training as mentioned above. First, unique PDE solutions are determined by the IC/BC, which cannot\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n17\n\nTable 2\nConverged loss with different penalty coefficient λ.\nλ = 10\nLoss\n2 × 10−4\n3 × 10−3\n\nBoundary condition\nx-Momentum equation\n\nλ = 1\n1 × 10−3\n1 × 10−4\n\nλ = 100\n7 × 10−6\n8 × 10−3\n\nλ = 1000\n1 × 10−7\n1 × 10−2\n\nFig. 12. Learning curves of different activation functions on a stenotic flow (A = 5 × 10−3, ν = 1 × 10−3) with (a) hard BC constraint, (b)\nsoft BC constraint with the penalty coefficient λ = 1.\n\nbe guaranteed by simply penalizing the boundary loss. Moreover, it is difficult to assign the relative weight (λ) for\ndifferent components in the loss function, and there could be “competing” effects between the equation loss and\nboundary loss, which makes the optimization difficult to converge. As shown in Table 2, by assigning a large weight\nfor the boundary loss (λ = 1000), the boundary condition can be well prescribed but the PDE residual remains a\nlarge value and cannot be further reduced. On the other hand, when the weight for the boundary is small (λ = 1),\nthe loss of x-momentum equation can be reduced to O(10−4) but the BC fails to be imposed. It is important to\nnote that none of these four λ leads to a physical stenotic flow pattern as shown in Fig. 11.\n\n4.2. Role of activation function in physics-constrained learning\n\nThe performance of DNN training is affected by the activation function to a large degree. The widely used\nactivation functions include ReLU, Sigmoid, Tanh, etc. [75]. However, these activation functions are not guaranteed\nto be optimized in terms of the convergence rate and accuracy. Recent studies [76,77] proposed to train an\nadaptive activation function as well as the neural network weights to achieve better convergence property. Notably,\nRamachandran et al. [76] introduced an adaptive activation function called Swish, which is defined as x·Sigmoid(βx)\nand β is a trainable parameter. Jagtap and Karniadakis [77] presented a new adaptive function defined as Tanh(nax),\nwhere a is an adaptive parameter to be learned and n is a scale factor that potentially speeds up the convergence.\nIn current work, the training process uses a Swish activation function with fixed β = 1. It is not clear how\nthe adaptivity of activation function can affect the convergence rate and accuracy. Furthermore, as we discussed\nand highlighted the necessity of the hard BC enforcement in the physics-constrained data-free learning, it is also\ninteresting to compare the relative importance of boundary condition enforcement and adaptive activation function\non model performance. Therefore, we test the effects of different activation functions with/without hard boundary\nconstraints in the stenosis case (A = 5 × 10−3, ν = 1 × 10−3), and the resulting learning curves are shown in\nFig. 12, where panels (a) and (b) show the convergence histories of different activation functions with and without\na hard BC enforcement, respectively.\n\nThe legend “Swish-β” refers to the Swish function with adaptive β, while the “Swish” means the Swish function\nwith a fixed β = 1. The results of adaptive Tanh activation function with different n [77] are plotted as well. It\n\n\f18\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nFig. 13. Performance of the proposed DNN for solving an inviscid Burgers equation, with a smooth initial solution turning into a shock at\nt = 0.5.\n\ncan be seen in Fig. 12a that the convergence rate of using adaptive Tanh functions is faster than that of the Swish\nactivation functions within the first 400 epochs, but the training loss of all cases finally converge to the same\norder. Moreover, the convergence curves almost overlap with each other for the same type of activation function\nwith different hyperparameters. When the BC is imposed in a soft manner (Fig. 12b), the solution can be quickly\ntrapped in a bad local minimum as we discussed above and the final training loss is over one-order larger than\nthat of the cases with hard BC constraints. Using different adaptive activation functions does not help to further\ndecrease the loss and all the convergence curves are overlapped. Moreover, the convergence history of each trainable\nhyperparameter (i.e., β in Swish function and a in Tanh function) is monitored (see Fig. 14 in Appendix C), and we\nfound that these hyperparameters are more likely to converge to the optimized values when the BCs are enforced in\na hard manner. To sum up, the way of imposing BCs is found to be more critical than the adaptivity of activation\nfunction in our cases in terms of accuracy in the physics-constrained data-free learning.\n\n4.3. Physics-constrained learning for discontinuous solution\n\nIn this work, we mainly focused on incompressible fluid problems relevant to hemodynamics applications,\nwhere the flow solutions are usually smooth and continuous. To better explore the performance of the proposed\nIC/BC-encoded physics-constrained learning, canonical problems with discontinuous solutions were studied as well.\nSpecifically, the proposed method was applied to capture the solutions of hyperbolic PDEs with discontinuities,\ne.g., inviscid Burger’s equation, which is known to be notoriously difficult for most traditional numerical methods,\nespecially the continuous Galerkin finite element methods. As an example, here we show the learning results for a\none-dimensional inviscid Burger’s equation,\n\n∂u\n∂t\n\n+ u\n\n∂u\n∂ x\n\n= 0,\n\nwith a smooth initial condition as,\n\nu0 =\n\n1\nπ sin(2π x), x ∈ [0, 1].\n\n(18)\n\n(19)\n\nThe analytical solution is available, where the shock starts to form at t = 0.5 (i.e., ∂u\n∂ x\n\n→ ∞ at x = 0.5) and\nbecomes very sharp at t = 1.0, as shown in Fig. 13. These features can be well captured by our PDE-constrained\nDNN, where the IC and periodic BC were encoded into the DNN structure in a hard manner. It can be observed\nthat even the sharpest discontinuity shown in Fig. 13c can be accurately predicted as the DNN approximation (blue\ndots) is almost overlapped with the exact solution (red line). The mean square test error for this case is 3.64 × 10−6,\nand the training loss can be reduced to O(10−6). The results demonstrate the good potential of physics-constrained\nlearning for solving PDEs even with discontinuous behaviors. However, the loss function formulation here can be\ninterpreted as a strong form of the PDE, and we expect that discontinuous Galerkin (DG) formulation in a weak\nform may help to further improve the learning performance for discontinuous problems, which is out of the scope\nof this work.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n19\n\nFig. 14. Convergence histories of trainable parameters (β for Swish and na for Tanh) of adaptive activation functions, where panels (a)–(e)\nare for cases with hard BC enforcement and panels (f)–(j) are for cases with soft BC enforcement. The legend a1, a2, a3 refer to three\ndifferent layers.\n\n4.4. Physics-constrained data-free learning vs. traditional data-driven learning\n\nWe have demonstrated that the solutions of parametric Navier–Stokes equations can be effectively learned by\nsolely minimizing the PDE residuals without using any simulation data. To better evaluate the advantages and\nlimitations of the physics-constrained learning, we also conducted a series of comparison studies between the\nphysics-constrained data-free DNN and traditional data-driven DNN in terms of learning efficiency and prediction\naccuracy. Namely, all the vascular flow cases discussed above are learned again in a purely data-driven way, where\nthe DNN architecture and hyperparameters remain the same and only the physics-based loss function (i.e., Eq. (10))\nis replaced with the data-based one (i.e., Eq. (5)). A number of CFD simulations with different input parameters are\nconducted and the simulated velocity and pressure fields are collected as the training data. The detailed comparisons\nfor stenotic and aneurysmal flows cases with varying viscosity and geometry are summarized by Tables 4 and\n5 in Appendix B. In general, the prediction results from purely data-driven learning are slightly more accurate\nthan those of physics-constrained learning, and the accuracy of both models improves as the parameter collocation\npoints increase. However, data-driven learning requires additional offline CFD simulations and this computational\noverhead can quickly grow as more training points are sampled from the parameter space. In this paper, since the\nCFD cases considered here are not costly to simulate (e.g., each CFD simulation takes about 40 CPU seconds), the\ncomputational overhead due to the offline data generation process is not significant. Nonetheless, the advantage of\nthe data-free feature in physics-constrained learning will become more notable when large-scale 3D flow problems\nare considered, where a single simulation run could be very expensive. It notes that when training data are ready for\nuse, the cost of data-driven training process is approximately similar to that of the physics-constrained training (in the\nsame order), though the latter is slightly slower due to the additional AD calculations for derivatives. To reduce the\ntraining cost in data-driven learning, one way is to reduce the spatial dimensionality by projecting the training data\n(i.e., velocity/pressure fields) onto the POD basis, and learning is performed on POD coefficients instead of spatial\ncollocation points. We also conducted POD-based data-driven learning and found that the learning performance\nwith the current shallow network structure is unsatisfactory (results are not shown here for conciseness).\n\nAdmittedly, the current form of physics-constrained DNN has its limitations, for example, the offline training\nprocess is still costly, the convergence cannot be guaranteed due to the non-convexity of DNN optimization, and\nscalability for high-dimensional complex problems is still challenging. The proposed PDE-constrained DNN is not\nexpected to replace the classical CFD (numerical) solvers, which have been developed for decades. However, the\ndevelopment of PDE-constrained DNN for surrogate modeling shows strong promise. Particularly, the proposed\nmethod is mesh-free and thus does not require arduous mesh generation labor and intensive domain expertise\nin numerical modeling, which is suitable for, e.g., rapidly testing ideas in the design phase. We expect that the\n\n\f20\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nTable 3\nSummary of test errors on ν-varying stenosis case for different DNN architectures, i.e., networks\nwith different number of hidden layers and neurons.\n\n1 Hidden layer\n\nNeurons per layer\n\nTest error eu\nTest error ev\n\nTest error e p\n\n2 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n3 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\n\nTest error e p\n\n4 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n5 Hidden layers\n\nNeurons per layer\n\nTest error eu\nTest error ev\nTest error e p\n\n10\n7.44 × 10−3\n9.31 × 10−6\n3.26 × 10−3\n\n10\n1.42 × 10−3\n1.05 × 10−6\n2.14 × 10−4\n\n10\n9.91 × 10−5\n9.71 × 10−8\n1.96 × 10−5\n\n10\n6.75 × 10−5\n5.33 × 10−8\n8.85 × 10−6\n\n10\n2.82 × 10−5\n6.12 × 10−8\n4.00 × 10−5\n\n20\n5.92 × 10−3\n6.83 × 10−6\n1.75 × 10−3\n\n20\n8.92 × 10−4\n6.23 × 10−7\n1.48 × 10−4\n\n20\n4.16 × 10−5\n3.72 × 10−8\n2.36 × 10−5\n\n20\n3.35 × 10−5\n2.23 × 10−8\n3.22 × 10−6\n\n20\n1.32 × 10−5\n3.04 × 10−8\n1.54 × 10−5\n\n30\n5.39 × 10−3\n5.7 × 10−6\n1.46 × 10−3\n\n30\n8.82 × 10−4\n6.72 × 10−7\n1.61 × 10−4\n\n30\n2.91 × 10−5\n3.52 × 10−8\n8.40 × 10−6\n\n30\n2.53 × 10−5\n1.86 × 10−8\n2.56 × 10−6\n\n30\n2.50 × 10−5\n7.55 × 10−8\n1.19 × 10−5\n\n40\n5.93 × 10−3\n7.08 × 10−6\n1.53 × 10−3\n\n40\n2.75 × 10−4\n2.91 × 10−7\n6.10 × 10−5\n\n40\n1.21 × 10−4\n3.67 × 10−8\n1.05 × 10−5\n\n40\n6.05 × 10−5\n7.32 × 10−8\n6.65 × 10−6\n\n40\n2.00 × 10−5\n2.86 × 10−8\n5.70 × 10−6\n\neffectiveness of surrogate model based on physics-constrained DNN will be significantly promoted along with the\npotential improvement of DNN training efficiency, e.g., a recent study has suggested that a novel photonic chip has\na potential to be used to train deep neural networks 10-million of times more efficiently than current CPUs/GPUs\ndo [78].\n\n5. Conclusion\n\nSurrogate modeling of fluid flows governed by the Navier–Stokes equations is significant for uncertainty\nquantification, optimization design, and inverse analysis in many engineering systems. As a universal function\napproximator, DNN is becoming a popular approach for surrogate modeling. However, training of a DNN often\nrequires large number of labeled data, which are usually not available for efficiently developing surrogates since each\ndata point requires an expensive CFD simulation. This paper presented a novel DNN surrogate for fluid simulations\nwithout using any labeled data (i.e., CFD simulation data). Specifically, a structured DNN architecture is devised\nto approximate the solutions of the parametric Navier–Stokes equations, where the initial/boundary conditions are\nsatisfied automatically. Instead of using any simulation data, the DNN is trained by solely minimizing the violation of\nthe mass and momentum conservation laws for fluid flows. Compared to the previous works of physics-constrained\nlearning, this paper focuses on modeling of fluid systems governed by parametric Navier–Stokes equations. The\nproposed methods were tested on three flow cases relevant to cardiovascular applications, i.e., circular pipe flow,\nstenotic flow, and aneurysmal flow. The DNNs with equation-based loss were trained to learn the flow fields with\nparameter variations in, e.g., viscosity and domain geometry. Uncertainties in these parameters are propagated\nthrough the trained DNN surrogate and the results are validated against the CFD benchmarks. The comparisons\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n21\n\nTable 4\nTraining and testing performance for stenosis case with varying viscosity/geometry.\n\nStenosis with varying viscosity (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n7598 s\n3.11 × 10−4\n4.13 × 10−4\n3.32 × 10−7\n6.08 × 10−5\n\n12977 s\n8 × 10−5\n8.18 × 10−5\n7.14 × 10−8\n2.33 × 10−5\n\n25988 s\n7.64 × 10−6\n6.02 × 10−5\n6.43 × 10−8\n9.82 × 10−6\n\n28543 s\n4.6 × 10−5\n3.29 × 10−5\n3.86 × 10−8\n6.87 × 10−6\n\n31996 s\n5.8 × 10−5\n7.27 × 10−5\n1.41 × 10−7\n1.41 × 10−5\n\nStenosis with varying viscosity (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6900 s\n400 s\n5 × 10−7\n1.49 × 10−5\n7.84 × 10−8\n6.42 × 10−7\n\n11967 s\n800 s\n5 × 10−7\n6.83 × 10−6\n2.37 × 10−7\n7.13 × 10−7\n\n19982 s\n1200 s\n1 × 10−7\n2.43 × 10−6\n8.84 × 10−8\n1.56 × 10−6\n\n22497 s\n1600 s\n1 × 10−7\n1.33 × 10−6\n3.09 × 10−8\n3.72 × 10−7\n\n30893 s\n2000 s\n1 × 10−7\n2.64 × 10−6\n3.01 × 10−8\n1.65 × 10−6\n\nStenosis with varying geometry (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6458 s\n5.0 × 10−4\n1.54 × 10−3\n2.93 × 10−6\n3.85 × 10−3\n\n11688 s\n1.0 × 10−3\n9.61 × 10−4\n1.76 × 10−6\n2.23 × 10−3\n\n18239 s\n3.0 × 10−4\n5.24 × 10−4\n1.26 × 10−6\n1.89 × 10−3\n\n24674 s\n7.0 × 10−4\n3.46 × 10−4\n7.43 × 10−7\n1.57 × 10−3\n\n42814 s\n1.0 × 10−4\n4.81 × 10−4\n9.53 × 10−7\n1.33 × 10−3\n\nStenosis with varying geometry (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n5498 s\n400 s\n3 × 10−7\n3.21 × 10−5\n2.11 × 10−7\n2.72 × 10−6\n\n11238 s\n800 s\n1 × 10−7\n2.18 × 10−5\n1.37 × 10−7\n2.88 × 10−7\n\n16285 s\n1200 s\n2 × 10−7\n2.4 × 10−5\n6.92 × 10−8\n1.22 × 10−6\n\n21848 s\n1600 s\n1 × 10−7\n2.74 × 10−5\n9.33 × 10−8\n3.41 × 10−7\n\n26396 s\n2000 s\n1 × 10−7\n3.12 × 10−5\n5.21 × 10−8\n6.40 × 10−7\n\nindicate the excellent agreement between the physics-constrained DNN surrogate models and CFD simulations.\nWithout using any labeled data in training, the DNN is able to accurately parameterize the velocity/pressure solutions\nwith varying viscosity and geometries, which can be used to efficiently propagate uncertainties with enormous MC\nsamples. Moreover, the performances of using hard and soft IC/BC enforcement approaches are compared and the\nissues of soft constraints in physics-constrained are discussed. We also investigated the influence of state-of-art\nadaptive activation functions and compared the present labeled-data-free learning approach with traditional data-\ndriven learning approach in terms of accuracy and efficiency. In summary, the results have demonstrated the merit\nof the proposed method and suggest a great promise in developing DNN for surrogate fluid models without the\nneed for CFD simulation data.\n\nAcknowledgments\n\nLS gratefully acknowledge partial funding of graduate fellowship from China Scholarship Council (CSC) in this\neffort. JXW would acknowledge support from the National Science Foundation (NSF contract CMMI-1934300) and\nDefense Advanced Research Projects Agency (DARPA) under the Physics of Artificial Intelligence (PAI) program\n(contract HR00111890034). The authors would like to thank Dr. Nicholas Zabaras and Mr. Yinhao Zhu for their\nhelpful discussions during this work. The authors also thank the anonymous reviewers for their comments and\nsuggestions, which helped improve the quality and clarity of the manuscript.\n\n\f22\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\nTable 5\nTraining and testing performance for aneurysm case with varying viscosity/geometry.\n\nAneurysm with varying viscosity (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n5731 s\n9.71 × 10−5\n2.72 × 10−4\n2.22 × 10−7\n6.14 × 10−7\n\n14490 s\n5.50 × 10−5\n9.2 × 10−5\n1.33 × 10−7\n1.18 × 10−5\n\n25350 s\n1.07 × 10−5\n8.33 × 10−5\n1.31 × 10−7\n5.81 × 10−6\n\n24810 s\n2.33 × 10−5\n6.4 × 10−5\n5.96 × 10−8\n1.12 × 10−5\n\n31595 s\n2.22 × 10−5\n4.67 × 10−5\n7.46 × 10−8\n4.75 × 10−6\n\nAneurysm with varying viscosity (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6851 s\n400 s\n5 × 10−7\n1.76 × 10−4\n1.07 × 10−7\n6.14 × 10−7\n\n11861 s\n800 s\n5 × 10−7\n5.11 × 10−6\n6.31 × 10−8\n2.38 × 10−7\n\n24580 s\n1200 s\n1 × 10−7\n8.51 × 10−6\n1.56 × 10−7\n1.81 × 10−6\n\n24353 s\n1600 s\n1 × 10−7\n5.30 × 10−7\n6.95 × 10−8\n1.06 × 10−6\n\n31116 s\n2000 s\n1 × 10−7\n3.20 × 10−6\n7.42 × 10−8\n3.28 × 10−7\n\nAneurysm with varying geometry (physics-constrained data-free)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n7755 s\n8.8 × 10−5\n3.07 × 10−4\n1.67 × 10−6\n3.77 × 10−5\n\n12904 s\n2.9 × 10−5\n1.38 × 10−4\n1.15 × 10−6\n2.09 × 10−5\n\n24645 s\n1.8 × 10−5\n1.29 × 10−4\n1.01 × 10−6\n1.28 × 10−5\n\n26724 s\n7.8 × 10−6\n1.31 × 10−4\n9.95 × 10−7\n8.99 × 10−6\n\n29515 s\n1.9 × 10−5\n1.19 × 10−4\n9.06 × 10−7\n1.02 × 10−5\n\nAneurysm with varying geometry (purely data-driven)\n\nTraining viscosity points\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining cost on GPU\nSimulation cost on CPU\n\nTraining minibatch loss\n\nTest error eu\nTest error ev\nTest error e p\n\n6322 s\n400 s\n3 × 10−7\n7.9 × 10−5\n8.77 × 10−7\n1.06 × 10−5\n\n11607 s\n800 s\n2 × 10−7\n8.63 × 10−5\n5.34 × 10−7\n8.94 × 10−6\n\n18582 s\n1200 s\n4 × 10−7\n7.74 × 10−5\n5.03 × 10−7\n8.96 × 10−6\n\n23309 s\n1600 s\n1 × 10−7\n8.69 × 10−5\n6.24 × 10−7\n9.24 × 10−6\n\n32124 s\n2000 s\n8 × 10−8\n6.73 × 10−5\n6.97 × 10−7\n1.26 × 10−5\n\nAppendix A. Learning performance of different network structures\n\nIn order to search for a network with the “simplest structure” that has the desired predictive accuracy for better\ntraining efficiency and generalizability, a series of tests on the ν-varying stenosis case were conducted using a group\nof DNNs with different number of hidden layers and neurons, from 1 hidden layer of 10 neurons per layer to 5\nhidden layers of 40 neurons per layer. The test errors for u, v and p of all the different networks are listed in Table 3.\nIn general, the test error decreases when the network becomes deeper (more hidden layers) and wider (more neurons\nper layer). We can see if the number of hidden layers is less than 3, adding one more layer can significantly reduce\nthe error by nearly an order of magnitude. However, the accuracy stays in the same order when the networks have\nmore than 3 layers. Furthermore, in the case of 3 hidden layers, there is no significant improvement by using more\nthan 20 hidden neurons per layer.\n\nAppendix B. Performance on learning idealized blood flows\n\nSee Tables 4 and 5.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n23\n\nAppendix C. Convergence of trainable parameters in activation functions\n\nFig. 14 shows the convergence histories of trainable parameters of different adaptive activation functions. The\ncomparison is also made between cases where the BCs are imposed in hard and soft manners. It can be found\nthat the parameters of activation function are more likely to converge with hard BC enforcement (panels (a)–(e))\ncompared to the cases with soft BC enforcement (panels (f)–(j)).\n\nReferences\n\n[1] J.D. Anderson, J. Wendt, Computational Fluid Dynamics, Vol. 206, Springer, 1995.\n[2] P. Benner, S. Gugercin, K. Willcox, A survey of projection-based model reduction methods for parametric dynamical systems, SIAM\n\nRev. 57 (4) (2015) 483–531.\n\n[3] P. Benner, A. Cohen, M. Ohlberger, K. Willcox, Model Reduction and Approximation: Theory and Algorithms, Vol. 15, SIAM, 2017.\n[4] T. Lassila, A. Manzoni, A. Quarteroni, G. Rozza, Model order reduction in fluid dynamics: challenges and perspectives, in: Reduced\n\nOrder Methods for Modeling and Computational Reduction, Springer, 2014, pp. 235–273.\n\n[5] C. Huang, K. Duraisamy, C. Merkle, Challenges in reduced order modeling of reacting flows, in: 2018 Joint Propulsion Conference,\n\n2018, p. 4675.\n\n[6] S. Chaturantabut, D.C. Sorensen, Nonlinear model reduction via discrete empirical interpolation, SIAM J. Sci. Comput. 32 (5) (2010)\n\n2737–2764.\n\n[7] B. Peherstorfer, Model reduction for transport-dominated problems via online adaptive bases and adaptive sampling, arXiv preprint ar\n\nXiv:1812.02094.\n\n[8] B. Peherstorfer, Z. Drmaˇc, S. Gugercin, Stabilizing discrete empirical interpolation via randomized and deterministic oversampling,\n\narXiv preprint arXiv:1808.10473.\n\n[9] W.-X. Ren, H.-B. Chen, Finite element model updating in structural dynamics by using the response surface method, Eng. Struct. 32\n\n(8) (2010) 2455–2465.\n\n[10] R.G. Regis, C.A. Shoemaker, A stochastic radial basis function method for the global optimization of expensive functions, INFORMS\n\nJ. Comput. 19 (4) (2007) 497–509.\n\n[11] M.C. Kennedy, A. O’Hagan, Predicting the output from a complex computer code when fast approximations are available, Biometrika\n\n87 (1) (2000) 1–13.\n\n[12] S. Atkinson, N. Zabaras, Structured bayesian gaussian process latent variable model: Applications to data-driven dimensionality reduction\n\nand high-dimensional inversion, J. Comput. Phys. 383 (2019) 166–195.\n\n[13] D. Xiu, G.E. Karniadakis, The Wiener–Askey polynomial chaos for stochastic differential equations, SIAM J. Sci. Comput. 24 (2)\n\n(2002) 619–644.\n\n[14] H.N. Najm, Uncertainty quantification and polynomial chaos techniques in computational fluid dynamics, Annu. Rev. Fluid Mech. 41\n\n(2009) 35–52.\n\n[15] X. Yang, X. Wan, L. Lin, L. Huan, A general framework for enhancing sparsity of generalized polynomial chaos expansions, Int. J.\n\nUncertain. Quantif. 9 (3) (2017) 221–243.\n\n[16] O. Le Maître, O.M. Knio, Spectral Methods for Uncertainty Quantification: With Applications to Computational Fluid Dynamics,\n\nSpringer Science & Business Media, 2010.\n\n[17] J.-X. Wang, C.J. Roy, H. Xiao, Propagation of input uncertainty in presence of model-form uncertainty: a multifidelity approach for\ncomputational fluid dynamics applications, ASCE-ASME J. Risk Uncertain. Eng. Syst. Part B: Mech. Eng. 4 (1) (2018) 011002.\n[18] Y. Zhu, N. Zabaras, Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification, J.\n\nComput. Phys. 366 (2018) 415–447.\n\n[19] R.K. Tripathy, I. Bilionis, Deep uq: Learning deep neural network surrogate models for high dimensional uncertainty quantification, J.\n\nComput. Phys. 375 (2018) 565–588.\n\n[20] S. Mo, N. Zabaras, X. Shi, J. Wu, Deep autoregressive neural networks for high-dimensional\n\ninverse problems in groundwater\n\ncontaminant source identification, Water Resour. Res. 55 (5) (2019) 3856–3881.\n\n[21] F. Scarselli, A.C. Tsoi, Universal approximation using feedforward neural networks: A survey of some existing methods, and some\n\nnew results, Neural Netw. 11 (1) (1998) 15–37.\n\n[22] M. Hutzenthaler, A. Jentzen, T. Kruse, T.A. Nguyen, P. von Wurstemberger, Overcoming the curse of dimensionality in the numerical\n\napproximation of semilinear parabolic partial differential equations, arXiv preprint arXiv:1807.01212.\n\n[23] P. Grohs, F. Hornung, A. Jentzen, P. Von Wurstemberger, A proof that artificial neural networks overcome the curse of dimensionality\n\nin the numerical approximation of black-scholes partial differential equations, arXiv preprint arXiv:1809.02362.\n\n[24] M. Hutzenthaler, A. Jentzen, T. Kruse, T.A. Nguyen, A proof that rectified deep neural networks overcome the curse of dimensionality\n\nin the numerical approximation of semilinear heat equations, arXiv preprint arXiv:1901.10854.\n\n[25] S. Lee, N. Baker, Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence, USDOE Office\n\nof Science (SC), United States, 2018.\n\n[26] G. Carleo, M. Troyer, Solving the quantum many-body problem with artificial neural networks, Science 355 (6325) (2017) 602–606.\n[27] J.-X. Wang, J.-L. Wu, H. Xiao, Physics-informed machine learning approach for reconstructing reynolds stress modeling discrepancies\n\nbased on dns data, Phys. Rev. Fluids 2 (3) (2017) 034603.\n\n[28] J. Ling, R. Jones, J. Templeton, Machine learning strategies for systems with invariance properties, J. Comput. Phys. 318 (2016) 22–35.\n\n\f24\n\nL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n[29] P.E. Shanahan, D. Trewartha, W. Detmold, Machine learning action parameters in lattice quantum chromodynamics, Phys. Rev. D 97\n\n(9) (2018) 094506.\n\n[30] R. King, O. Hennigh, A. Mohan, M. Chertkov, From deep to physics-informed learning of turbulence: Diagnostics, arXiv preprint ar\n\nXiv:1810.07785.\n\n[31] S.L. Brunton, J.N. Kutz, Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control, Cambridge\n\nUniversity Press, 2019.\n\n[32] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436.\n[33] M. Raissi, P. Perdikaris, G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse\n\nproblems involving nonlinear partial differential equations, J. Comput. Phys. 378 (2019) 686–707.\n\n[34] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., Deep\n\nspeech: Scaling up end-to-end speech recognition, arXiv preprint arXiv:1412.5567.\n\n[35] H. Lee, I.S. Kang, Neural algorithm for solving differential equations, J. Comput. Phys. 91 (1) (1990) 110–131.\n[36] I.E. Lagaris, A. Likas, D.I. Fotiadis, Artificial neural networks for solving ordinary and partial differential equations, IEEE Trans.\n\nNeural Netw. 9 (5) (1998) 987–1000.\n\n[37] I.E. Lagaris, A.C. Likas, D.G. Papageorgiou, Neural-network methods for boundary value problems with irregular boundaries, IEEE\n\nTrans. Neural Netw. 11 (5) (2000) 1041–1049.\n\n[38] M. Raissi, A. Yazdani, G.E. Karniadakis, Hidden fluid mechanics: A navier-stokes informed deep learning framework for assimilating\n\nflow visualization data, arXiv preprint arXiv:1808.04327.\n\n[39] M. Raissi, Z. Wang, M.S. Triantafyllou, G.E. Karniadakis, Deep learning of vortex-induced vibrations, J. Fluid Mech. 861 (2019)\n\n119–137.\n\n[40] A.M. Tartakovsky, C.O. Marrero, D. Tartakovsky, D. Barajas-Solano, Learning parameters and constitutive relationships with physics\n\ninformed deep neural networks, arXiv preprint arXiv:1808.03398.\n\n[41] X. Meng, G.E. Karniadakis, A composite neural network that learns from multi-fidelity data: Application to function approximation\n\nand inverse pde problems, arXiv preprint arXiv:1903.00104.\n\n[42] D. Zhang, L. Lu, L. Guo, G.E. Karniadakis, Quantifying total uncertainty in physics-informed neural networks for solving forward\n\nand inverse stochastic problems, arXiv preprint arXiv:1809.08327.\n\n[43] Y. Yang, P. Perdikaris, Adversarial uncertainty quantification in physics-informed neural networks, arXiv preprint arXiv:1811.04026.\n[44] R. Sharma, A.B. Farimani, J. Gomes, P. Eastman, V. Pande, Weakly-supervised deep learning of heat transport via physics informed\n\nloss, arXiv preprint arXiv:1807.11374.\n\n[45] M.A. Nabian, H. Meidani, Physics-informed regularization of deep neural networks, arXiv preprint arXiv:1810.05547.\n[46] K. Xu, E. Darve, The neural network approach to inverse problems in differential equations, arXiv preprint arXiv:1901.07758.\n[47] J.R. Holland, J.D. Baeder, K. Duraisamy, Towards integrated field inversion and machine learning with embedded neural networks for\n\nrans modeling, in: AIAA Scitech 2019 Forum, 2019, p. 1884.\n\n[48] R. Stewart, S. Ermon, Label-free supervision of neural networks with physics and domain knowledge, in: Thirty-First AAAI Conference\n\non Artificial Intelligence, 2017, pp. 2576–2582.\n\n[49] J. Sirignano, K. Spiliopoulos, Dgm: A deep learning algorithm for solving partial differential equations, J. Comput. Phys. 375 (2018)\n\n1339–1364.\n\n[50] J. Berg, K. Nyström, A unified deep artificial neural network approach to partial differential equations in complex geometries,\n\nNeurocomputing 317 (2018) 28–41.\n\n[51] E. Weinan, J. Han, A. Jentzen, Deep learning-based numerical methods for high-dimensional parabolic partial differential equations\n\nand backward stochastic differential equations, Commun. Math. Stat. 5 (4) (2017) 349–380.\n\n[52] C. Beck, E. Weinan, A. Jentzen, Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential\n\nequations and second-order backward stochastic differential equations, J. Nonlinear Sci. (2017) 1–57.\n\n[53] C. Beck, S. Becker, P. Grohs, N. Jaafari, A. Jentzen, Solving stochastic differential equations and Kolmogorov equations by means of\n\ndeep learning, arXiv preprint arXiv:1806.00421.\n\n[54] E. Weinan, B. Yu, The deep ritz method: A deep learning-based numerical algorithm for solving variational problems, Commun. Math.\n\nStat. 6 (1) (2018) 1–12.\n\n[55] J. Han, A. Jentzen, E. Weinan, Solving high-dimensional partial differential equations using deep learning, Proc. Natl. Acad. Sci. 115\n\n(34) (2018) 8505–8510.\n\n[56] M.A. Nabian, H. Meidani, A deep neural network surrogate for high-dimensional random partial differential equations, arXiv preprint\n\narXiv:1806.02957.\n\n[57] S. Karumuri, R. Tripathy, I. Bilionis, J. Panchal, Simulator-free solution of high-dimensional stochastic elliptic partial differential\n\nequations using deep neural networks, arXiv preprint arXiv:1902.05200.\n\n[58] Y. Zhu, N. Zabaras, P.-S. Koutsourelakis, P. Perdikaris, Physics-constrained deep learning for high-dimensional surrogate modeling and\n\nuncertainty quantification without labeled data, arXiv preprint arXiv:1901.06314.\n\n[59] A.G. Baydin, B.A. Pearlmutter, A.A. Radul, J.M. Siskind, Automatic differentiation in machine learning: a survey, J. Mach. Learn.\n\nRes. 18 (2018) 1–43.\n\n[60] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, A. Lerer, Automatic differentiation\n\nin PyTorch, in: NIPS Autodiff Workshop, 2017.\n\n[61] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: A system\nfor large-scale machine learning, in: 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 16, 2016,\npp. 265–283.\n\n\fL. Sun, H. Gao, S. Pan et al. / Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732\n\n25\n\n[62] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I.J. Goodfellow, A. Bergeron, N. Bouchard, Y. Bengio, Theano: new features and\nspeed improvements. deep learning and unsupervised feature learning, in: Neural Information Processing Systems Workshop (NIPS),\n2012, pp. 1–10.\n\n[63] R. Kleinberg, Y. Li, Y. Yuan, An alternative view: When does SGD escape local minima? arXiv preprint arXiv:1802.06175.\n[64] P. Márquez-Neila, M. Salzmann, P. Fua, Imposing hard constraints on deep networks: Promises and limitations, arXiv preprint arXiv:\n\n1706.02025.\n\n[65] P. Ramachandran, B. Zoph, Q.V. Le, Swish: a self-gated activation function, arXiv preprint arXiv:1710.05941.\n[66] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980.\n[67] X. Glorot, Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in: Proceedings of the International\n\nConference on Artificial Intelligence and Statistics, AISTATS’10, Society for Artificial Intelligence and Statistics, 2010.\n\n[68] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, in:\n\nProceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026–1034.\n\n[69] A. Blumer, A. Ehrenfeucht, D. Haussler, M.K. Warmuth, Occam’s razor, Inform. Process. Lett. 24 (6) (1987) 377–380.\n[70] H. Jasak, A. Jemcov, U. Kingdom, Openfoam: A c++ library for complex physics simulations, in: International Workshop on Coupled\n\nMethods in Numerical Dynamics, IUC, 2007, pp. 1–20.\n\n[71] S. Berger, L.-D. Jou, Flows in stenotic vessels, Annu. Rev. Fluid Mech. 32 (1) (2000) 347–382.\n[72] J.L. Brisman, J.K. Song, D.W. Newell, Cerebral aneurysms, New Engl. J. Med. 355 (9) (2006) 928–939.\n[73] J.R. Cebral, F. Mut, J. Weir, C.M. Putman, Association of hemodynamic characteristics and cerebral aneurysm rupture, Am. J.\n\nNeuroradiol. 32 (2) (2011) 264–270.\n\n[74] N. Chalouhi, B.L. Hoh, D. Hasan, Review of cerebral aneurysm formation, growth, and rupture, Stroke 44 (12) (2013) 3613–3622.\n[75] A.L. Maas, A.Y. Hannun, A.Y. Ng, Rectifier nonlinearities improve neural network acoustic models, in: Proceedings of the 30th Annual\n\nInternational Conference on Machine Learning, Vol. 30, 2013, p. 3.\n\n[76] P. Ramachandran, B. Zoph, Q.V. Le, Searching for activation functions, arXiv preprint arXiv:1710.05941.\n[77] A.D. Jagtap, G.E. Karniadakis, Adaptive activation functions accelerate convergence in deep and physics-informed neural networks,\n\narXiv preprint arXiv:1906.01170.\n\n[78] R. Hamerly, L. Bernstein, A. Sludds, M. Soljaˇci´c, D. Englund, Large-scale optical neural networks based on photoelectric multiplication,\n\nPhys. Rev. X 9 (2) (2019) 021032.\n\n"
  },
  "isError": false
}

thinking
**Composing concise structured summary**
codex
item_key: 2P33W7SZ

**Problem**
- CFD for parametric fluid flows is expensive, especially with sparse labeled data and changing geometries.

**Method**
- A physics-constrained, label-free DNN surrogate enforces initial/boundary conditions and minimizes Navier-Stokes residuals instead of fitting simulation labels.

**Results**
- Across pipe/stenosis/aneurysm cases, the surrogate matched CFD flow fields and uncertainty propagation with low reported test errors.

**Limitations**
- Evidence is limited to selected internal-flow benchmarks, and training still requires substantial GPU time.
tokens used
19,303
item_key: 2P33W7SZ

**Problem**
- CFD for parametric fluid flows is expensive, especially with sparse labeled data and changing geometries.

**Method**
- A physics-constrained, label-free DNN surrogate enforces initial/boundary conditions and minimizes Navier-Stokes residuals instead of fitting simulation labels.

**Results**
- Across pipe/stenosis/aneurysm cases, the surrogate matched CFD flow fields and uncertainty propagation with low reported test errors.

**Limitations**
- Evidence is limited to selected internal-flow benchmarks, and training still requires substantial GPU time.

